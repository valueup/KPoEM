{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the KPoEM Dataset and Model: Poetry Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic setup and library imports\n",
    "- Import Libraries and Set Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/bitsandbytes-0.45.4.dev0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/bitsandbytes-0.45.4.dev0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (23.2)\n",
      "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -q huggingface_hub langchain langchain-core langchain-community langchain-huggingface \n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, ElectraModel, AutoModelForCausalLM, pipeline\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the KPoEM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ì´ˆ ì„¸íŒ…\n",
    "REPO_ID = \"AKS-DHLAB/KPoEM\" # í—ˆê¹…í˜ì´ìŠ¤ì— ì—…ë¡œë“œëœ ê°ì •ë¶„ë¥˜ëª¨ë¸ id\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #GPU ì‚¬ìš©\n",
    "THRESH_HOLD = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPoEM_Classifier í´ë˜ìŠ¤\n",
    "class KPoEM_Classifier(nn.Module):\n",
    "    def __init__(self, repo_id, device):\n",
    "        self.labels = [\n",
    "            'ë¶ˆí‰/ë¶ˆë§Œ', 'í™˜ì˜/í˜¸ì˜', 'ê°ë™/ê°íƒ„', 'ì§€ê¸‹ì§€ê¸‹', 'ê³ ë§ˆì›€', 'ìŠ¬í””', 'í™”ë‚¨/ë¶„ë…¸', 'ì¡´ê²½',\n",
    "            'ê¸°ëŒ€ê°', 'ìš°ì­ëŒ/ë¬´ì‹œí•¨', 'ì•ˆíƒ€ê¹Œì›€/ì‹¤ë§', 'ë¹„ì¥í•¨', 'ì˜ì‹¬/ë¶ˆì‹ ', 'ë¿Œë“¯í•¨', 'í¸ì•ˆ/ì¾Œì ',\n",
    "            'ì‹ ê¸°í•¨/ê´€ì‹¬', 'ì•„ê»´ì£¼ëŠ”', 'ë¶€ë„ëŸ¬ì›€', 'ê³µí¬/ë¬´ì„œì›€', 'ì ˆë§', 'í•œì‹¬í•¨', 'ì—­ê²¨ì›€/ì§•ê·¸ëŸ¬ì›€',\n",
    "            'ì§œì¦', 'ì–´ì´ì—†ìŒ', 'ì—†ìŒ', 'íŒ¨ë°°/ìê¸°í˜ì˜¤', 'ê·€ì°®ìŒ', 'í˜ë“¦/ì§€ì¹¨', 'ì¦ê±°ì›€/ì‹ ë‚¨', 'ê¹¨ë‹¬ìŒ',\n",
    "            'ì£„ì±…ê°', 'ì¦ì˜¤/í˜ì˜¤', 'íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨)', 'ë‹¹í™©/ë‚œì²˜', 'ê²½ì•…', 'ë¶€ë‹´/ì•ˆ_ë‚´í‚´', 'ì„œëŸ¬ì›€',\n",
    "            'ì¬ë¯¸ì—†ìŒ', 'ë¶ˆìŒí•¨/ì—°ë¯¼', 'ë†€ëŒ', 'í–‰ë³µ', 'ë¶ˆì•ˆ/ê±±ì •', 'ê¸°ì¨', 'ì•ˆì‹¬/ì‹ ë¢°'\n",
    "        ]\n",
    "        num_labels = len(self.labels)\n",
    "        #ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(repo_id) \n",
    "        self.electra = AutoModel.from_pretrained(repo_id)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(self.electra.config.hidden_size, num_labels)\n",
    "        )\n",
    "\n",
    "        weights_path = hf_hub_download(repo_id=repo_id, filename=\"classifier_state.bin\") #ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "        self.classifier.load_state_dict(torch.load(weights_path, map_location=self.device))\n",
    "        self.to(self.device)\n",
    "        self.eval()\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ìµœì¢… logits ë°˜í™˜\n",
    "    def forward(self, text: str):\n",
    "        encoding = self.tokenizer(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=512,\n",
    "          padding=\"max_length\",\n",
    "          truncation=True,\n",
    "          return_tensors='pt',\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.electra(\n",
    "                input_ids=encoding[\"input_ids\"],\n",
    "                attention_mask=encoding[\"attention_mask\"],\n",
    "                token_type_ids=encoding[\"token_type_ids\"]\n",
    "            )\n",
    "\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "    def analyze(self, text: str, threshold=0):\n",
    "        logits = self.forward(text)\n",
    "        probabilities = torch.sigmoid(logits.squeeze()) #í™•ë¥ ë¡œ ë³€í™˜ â†’ threshold ì´ìƒì´ë©´ ì„ íƒ\n",
    "        predictions = (probabilities > threshold).int()\n",
    "\n",
    "        result_dict = {\n",
    "            self.labels[i]: float(round(probabilities[i].item(), 3))\n",
    "            for i, label_id in enumerate(predictions)\n",
    "            if label_id == 1\n",
    "        }\n",
    "\n",
    "        # í™•ë¥ ê°’ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ëœ dictë¡œ ë°˜í™˜\n",
    "        result_dict = dict(sorted(result_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "        return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... 'cuda' í™˜ê²½ì—ì„œ 'AKS-DHLAB/KPoEM' ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤ ...\n",
      "KPoEM ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí•˜ì˜€ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# KPoEM ëª¨ë¸ ë¡œë“œ\n",
    "print(f\"... '{DEVICE}' í™˜ê²½ì—ì„œ '{REPO_ID}' ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤ ...\")\n",
    "kpoem_model = KPoEM_Classifier(repo_id=REPO_ID, device=DEVICE)\n",
    "print(\"KPoEM ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí•˜ì˜€ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ê¸°ëŒ€ê°': 0.919,\n",
       " 'ê¸°ì¨': 0.809,\n",
       " 'ì¦ê±°ì›€/ì‹ ë‚¨': 0.635,\n",
       " 'í–‰ë³µ': 0.622,\n",
       " 'ë¹„ì¥í•¨': 0.511,\n",
       " 'í™˜ì˜/í˜¸ì˜': 0.495,\n",
       " 'ì•„ê»´ì£¼ëŠ”': 0.372,\n",
       " 'í¸ì•ˆ/ì¾Œì ': 0.32,\n",
       " 'ê°ë™/ê°íƒ„': 0.304}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëª¨ë¸ ì‚¬ìš© í…ŒìŠ¤íŠ¸\n",
    "test = \"ë¯¸í’ì— ì›ƒëŠ” ì•„ì¹¨ì„ ê¸°ì›í•˜ë ¨ë‹¤\"\n",
    "result = kpoem_model.analyze(test, threshold=THRESH_HOLD)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Downloading and Loading the LLM for Poetry Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3994dc7da494efea7d1dba38ec55606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. GPU/CPU ìë™ ì˜¤í”„ë¡œë”© í™œì„±í™”\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"K-intelligence/Midm-2.0-Base-Instruct\"\n",
    "CACHE_DIR = \"/home/work/KPoEM/code/AICreation/model\"\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "#ëª¨ë¸ ë¡œë“œ - device_map=\"auto\" ìœ ì§€ (ìë™ ë¶„ì‚° ë¡œë“œ)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",           # GPU + CPU ë©”ëª¨ë¦¬ ìë™ ë¶„ì‚°\n",
    "    low_cpu_mem_usage=True,      # ë¡œë”© ì‹œ CPU ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "print(\"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. GPU/CPU ìë™ ì˜¤í”„ë¡œë”© í™œì„±í™”\")\n",
    "\n",
    "# HuggingFace pipeline ìƒì„± (device ì„¤ì •í•˜ì§€ ì•Šì•„ë„ ìë™)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=512,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "# LangChain LLM ë˜í¼ë¡œ ê°ì‹¸ê¸°\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"ë‹¤ìŒ ì£¼ì œë¡œ ê°ì„±ì ì¸ ì‹œë¥¼ ì¨ì£¼ì„¸ìš”:\\nì£¼ì œ: {topic}\\n\\n### ì‹œ:\\n\"\n",
    ")\n",
    "\n",
    "# LLMChain ìƒì„±\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt-Based Poetry Generation 1\n",
    "Using Input Text and Emotion Classification Results (Without a Vector Database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê°ì • ë¶„ë¥˜ ëª¨ë¸ë§Œ ì ìš©í•˜ì—¬ LLMìœ¼ë¡œ ì‹œ ìƒì„±(Vector DB ë¯¸ì ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_poetry_section(template):\n",
    "    # Split the template by \"### ì‹œ:\" and extract the part after it\n",
    "    if \"### ì‹œ:\" in template:\n",
    "        poetry_section = template.split(\"### ì‹œ:\")[1].strip()\n",
    "        # Split by lines and return as a list\n",
    "        poetry_lines = poetry_section.splitlines()\n",
    "        return poetry_lines\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5ï¸âƒ£ ì „ì²´ íë¦„ í•¨ìˆ˜\n",
    "def emotion_to_poetry(sample_text): #ê°ì • ë¶„ë¥˜ì— ì‚¬ìš©í•  ì‚¬ìš©ì í…ìŠ¤íŠ¸ ì¸í’‹.\n",
    "    emotion_scores = kpoem_model.analyze(sample_text, threshold=0.3)\n",
    "    \n",
    "    template = \"\"\"\n",
    "    ### ì‹œìŠ¤í…œ:\n",
    "    ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ì‹œì ì¸ ì‘ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê°ì •ëª©ë¡ê³¼ ì •ë„ë¥¼ ë…¹ì—¬ë‚¸ ì§§ì€ í•œêµ­ì–´ ì‹œë¥¼ ì¨ì£¼ì„¸ìš”.\n",
    "    ì£¼ì–´ì§„ ê°ì • ëª©ë¡ì„ ìµœëŒ€í•œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì€ìœ ì™€ ìƒì§•ì„ ì‚¬ìš©í•˜ì—¬ ì°½ì˜ì ìœ¼ë¡œ ê°ì •ì„ í‘œí˜„í•˜ì„¸ìš”.\n",
    "    \n",
    "    ìƒì„±í•œ ì‹œë§Œ ì•Œë ¤ì£¼ì„¸ìš”. ê·¸ ì™¸ì— ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "    ### ê°ì • ëª©ë¡: {emotion}\n",
    "    ### ì‹œ:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"emotion\"],\n",
    "        template=template.strip()\n",
    "    )\n",
    "    # print(emotion_scores)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.run(emotion=emotion_scores)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"ë¯¸í’ì— ì›ƒëŠ” ì•„ì¹¨ì„ ê¸°ì›í•˜ë ¨ë‹¤\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6ï¸âƒ£ í…ŒìŠ¤íŠ¸\n",
    "generated_poem = emotion_to_poetry(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ´ ìƒì„±ëœ ì‹œ:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ë³„ë“¤ì´ ì¶¤ì¶”ëŠ” ë°¤,',\n",
       " '    ë°”ëŒì´ ì†ì‚­ì´ëŠ” ì•½ì† ì†ì—ì„œ',\n",
       " '    ìƒˆì‹¹ì²˜ëŸ¼ í”¼ì–´ë‚˜ëŠ” ì„¤ë ˜ì´ì—¬,',\n",
       " '    ê·¸ëŒ€ í–¥í•œ ê¸°ì¨ì˜ ë¬¼ê²°ì´',\n",
       " '    íŒŒë„ê°€ ë˜ì–´ ë°€ë ¤ì˜¤ë„¤.',\n",
       " '',\n",
       " '    ì´ ìˆœê°„ ëª¨ë“  ê²ƒì´ ë¹›ë‚˜ëŠ”êµ¬ë‚˜,',\n",
       " '    ë§ˆìŒ ê¹Šìˆ™ì´ ìŠ¤ë©°ë“œëŠ” ë”°ëœ»í•¨ì—',\n",
       " '    í¸ì•ˆí•¨ì´ë€ ì´ë¦„ì˜ í¬ê·¼í•œ ì´ë¶ˆë¡œ',\n",
       " '    í•˜ë£¨ë¥¼ ê°ì‹¸ ì•ˆê³  ìˆë„¤',\n",
       " '',\n",
       " '    ê²½ì´ë¡œì›€ì˜ ë¹›ì¤„ê¸° ë”°ë¼ê°€ë©°',\n",
       " '    ë‚˜ëŠ” ì˜¤ëŠ˜ë„ ì‚´ì•„ ìˆ¨ì‰¬ë…¸ë¼']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"ğŸ´ ìƒì„±ëœ ì‹œ:\\n\")\n",
    "extract_poetry_section(generated_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prompt-Based Poetry Generation 2\n",
    "Using Input Text and Emotion Classification Results (With a Vector Database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Loading the Vector Store Built with the KcELECTRA Backbone Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KcELECTRA ê¸°ë°˜ ì»¤ìŠ¤í…€ ì„ë² ë”© í´ë˜ìŠ¤ ì •ì˜\n",
    "class KcELECTRAEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str = \"beomi/KcELECTRA-base\", device: str = \"cpu\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def _embed(self, text: str):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        return cls_embedding.squeeze().cpu().numpy()\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        return [self._embed(text).tolist() for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        return self._embed(text).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3ï¸âƒ£ ë²¡í„° ì„ë² ë”© ëª¨ë¸ ë¡œë”© (í•œêµ­ì–´ ì§€ì›í•˜ëŠ” ëª¨ë¸ ê¶Œì¥) KcElectra -> backbone ëª¨ë¸ë¡œ ì‚¬ìš©\n",
    "embedding_model = KcELECTRAEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¡œì»¬ì—ì„œ ë¡œë“œ (ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” íŒŒì¼ì¼ ê²½ìš°)\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"./data/poetry_vectorstore\", #ê¹ƒí—ˆë¸Œ íŒŒì¼ì€ ë§í¬ ìˆ˜ì • \"./vectorstore\" ì°¸ê³  - https://github.com/AKS-DHLAB/KPoEM/blob/main/KPoEM_vectorstore.ipynb\n",
    "    embedding_model, \n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - After Computing Vector Similarity, \n",
    "Use the Emotion Information Stored in the Metadata of the Retrieved 100 Entries to Construct Context for Poetry Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒìœ„ ê°ì • ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n",
    "def get_top_emotions(scores, top_n=10):\n",
    "    \"\"\"ì ìˆ˜ dictì—ì„œ ìƒìœ„ Nê°œì˜ ê°ì •ì„ ì„ íƒí•˜ê³  key ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\"\"\"\n",
    "    top_emotions = dict(sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n])\n",
    "    return top_emotions, list(top_emotions.keys())\n",
    "\n",
    "# ìƒìœ„ ê°ì •ê³¼ ë²¡í„°DBì˜ ë©”íƒ€ë°ì´í„°ì˜ \"emotion\"ê°’ì„ ë¹„êµí•˜ì—¬ ê°€ì¥ ìœ ì‚¬í•œ ì‹œêµ¬ 10ê°œë¥¼ ì„ íƒ\n",
    "def filter_context_by_emotion(context, top_emotion_keys, top_k=10):\n",
    "    \"\"\"\n",
    "    context(Document ë¦¬ìŠ¤íŠ¸)ì—ì„œ ê°ì • êµì§‘í•© ê°œìˆ˜ë¡œ í•„í„°ë§ ë° ì •ë ¬.\n",
    "    top_emotion_keysì™€ ê²¹ì¹˜ëŠ” ê°ì •ì´ ìˆëŠ” ë¬¸ì„œë§Œ ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    results_with_score = []\n",
    "    for doc in context:\n",
    "        doc_emotions = set(doc.metadata.get(\"emotion\", {}).keys())\n",
    "        overlap = doc_emotions.intersection(top_emotion_keys)\n",
    "        if overlap:  # í•˜ë‚˜ë¼ë„ ê²¹ì¹˜ë©´\n",
    "            results_with_score.append((doc, len(overlap), overlap))\n",
    "\n",
    "    # êµì§‘í•© ê°œìˆ˜ ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ top_k ì„ íƒ\n",
    "    results_with_score.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, _, _ in results_with_score[:top_k]]\n",
    "\n",
    "def build_prompt_template():\n",
    "    \"\"\"ì‹œ ìƒì„± í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\"\"\"\n",
    "    return PromptTemplate(\n",
    "        input_variables=[\"top_emotion\", \"context_snippets\"],\n",
    "        template=\"\"\"\n",
    "        ### ì‹œìŠ¤í…œ:\n",
    "        ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ê°ì„±ì ì¸ ê·¼í˜„ëŒ€ ì‹œì¸ì…ë‹ˆë‹¤. \n",
    "        ë‹¤ìŒ ê°ì •ëª©ë¡ì— ì–¸ê¸‰ëœ ê°ì •ë“¤ì„ ì£¼ëœ ì‹œì˜ ì •ì„œë¡œ í™œìš©í•˜ì„¸ìš”.\n",
    "        ì˜ì–´ë‚˜ ë‹¤ë¥¸ ì–¸ì–´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ê³ , í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”. \n",
    "        ì´ëª¨ì§€ë‚˜ ê·¸ë¦¼ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "        {context_snippets} \n",
    "        ìœ„ ë¬¸ì¥ë“¤ì—ì„œ ì˜›ìŠ¤ëŸ¬ìš´ í•œêµ­ ê³ ìœ ì˜ í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ì‹œë¥¼ í•˜ë‚˜ ì§€ì–´ì£¼ì„¸ìš”.\n",
    "        ì€ìœ ì™€ ìƒì§•ì„ ì‚¬ìš©í•˜ì—¬ ì°½ì˜ì ìœ¼ë¡œ ê°ì •ì„ í‘œí˜„í•˜ì„¸ìš”. \n",
    "        ì‹œ í•´ì„¤ì€ í•„ìš”ì—†ìŠµë‹ˆë‹¤. ì‹œë§Œ ì¨ì£¼ì„¸ìš”.\n",
    "        ê°ì • ëª©ë¡: {top_emotion}\n",
    "        ### ì‹œ:\n",
    "        \"\"\".strip()\n",
    "    )\n",
    "\n",
    "def emotion_to_poetry_V(user_input, top_n=10, context_k=100, filtered_k=10):\n",
    "    \"\"\"ì „ì²´ íŒŒì´í”„ë¼ì¸: ê°ì • ë¶„ì„ â†’ ê´€ë ¨ ì‹œêµ¬ ê²€ìƒ‰ â†’ ì‹œ ìƒì„±\"\"\"\n",
    "    # 1. ê°ì • ë¶„ì„\n",
    "    scores = kpoem_model.analyze(user_input, threshold=0.3) #ì¸í’‹ í…ìŠ¤íŠ¸ ê°ì •ë¶„ë¥˜\n",
    "    top_emotions, top_emotion_keys = get_top_emotions(scores, top_n=top_n) #ìƒìœ„ ê°ì • ë¼ë²¨ ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "    # 2. ìœ ì‚¬ ë¬¸ë§¥ ê²€ìƒ‰\n",
    "    context = vectorstore.similarity_search(user_input, k=context_k) # ì¸í’‹ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ë²¡í„° ìœ ì‚¬ë„ê°€ ê·¼ì ‘í•œ VecotreDBì— ì €ì¥ëœ ë°ì´í„° í˜¸ì¶œ(100ê°œë¡œ ì„¸íŒ…)\n",
    "    filtered_results = filter_context_by_emotion(context, top_emotion_keys, top_k=filtered_k) # ê°ì • ê°’ë„ ë¹„ìŠ·í•œ ë°±í„°ìœ ì‚¬ë„ ê·¼ì  ì‹œêµ¬ í•„í„°ë§\n",
    "\n",
    "    # 3. ë¬¸ë§¥ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "    context_text = \"\\n\".join([doc.page_content for doc in filtered_results]) #í”„ë¡¬í”„íŠ¸ì—”ì§€ë‹ˆì–´ë§ì— ì‚¬ìš©í•  ë¬¸ë§¥ í…ìŠ¤íŠ¸ ìƒì„±(RAG)\n",
    "\n",
    "    # 4. LLM í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ì‹¤í–‰\n",
    "    prompt = build_prompt_template()\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.run(top_emotion=top_emotions, context_snippets=context_text)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryGenerator:\n",
    "    \"\"\"ê°ì • ë¶„ì„ + ë¬¸ë§¥ ê²€ìƒ‰ + ì‹œ ìƒì„±ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, kpoem_model, vectorstore, llm):\n",
    "        \"\"\"\n",
    "        í´ë˜ìŠ¤ ì´ˆê¸°í™”\n",
    "        Args:\n",
    "            kpoem_model: KPoEM ê°ì • ë¶„ë¥˜ ëª¨ë¸\n",
    "            vectorstore: FAISS ë“± ë²¡í„°ìŠ¤í† ì–´ ê°ì²´\n",
    "            llm: LangChain LLM ê°ì²´\n",
    "        \"\"\"\n",
    "        self.kpoem_model = kpoem_model\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "    # ìƒìœ„ ê°ì • ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n",
    "    def get_top_emotions(self, scores, top_n=10):\n",
    "        \"\"\"ì ìˆ˜ dictì—ì„œ ìƒìœ„ Nê°œì˜ ê°ì •ì„ ì„ íƒí•˜ê³  key ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\"\"\"\n",
    "        top_emotions = dict(sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n])\n",
    "        return top_emotions, list(top_emotions.keys())\n",
    "    # ìƒìœ„ ê°ì •ê³¼ ë²¡í„°DBì˜ ë©”íƒ€ë°ì´í„°ì˜ \"emotion\"ê°’ì„ ë¹„êµí•˜ì—¬ ê°€ì¥ ìœ ì‚¬í•œ ì‹œêµ¬ 10ê°œë¥¼ ì„ íƒ\n",
    "    def filter_context_by_emotion(self, context, top_emotion_keys, top_k=10):\n",
    "        \"\"\"context(Document ë¦¬ìŠ¤íŠ¸)ì—ì„œ ê°ì • êµì§‘í•© ê°œìˆ˜ë¡œ í•„í„°ë§ ë° ì •ë ¬.\"\"\"\n",
    "        results_with_score = []\n",
    "        for doc in context:\n",
    "            doc_emotions = set(doc.metadata.get(\"emotion\", {}).keys())\n",
    "            overlap = doc_emotions.intersection(top_emotion_keys)\n",
    "            if overlap:\n",
    "                results_with_score.append((doc, len(overlap), overlap))\n",
    "        # êµì§‘í•© ê°œìˆ˜ ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ top_k ì„ íƒ\n",
    "        results_with_score.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, _, _ in results_with_score[:top_k]]\n",
    "\n",
    "    def build_prompt_template(self):\n",
    "        \"\"\"ì‹œ ìƒì„± í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\"\"\"\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"top_emotion\", \"context_snippets\"],\n",
    "            template=\"\"\"\n",
    "            ### ì‹œìŠ¤í…œ:\n",
    "            ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ê°ì„±ì ì¸ ê·¼í˜„ëŒ€ ì‹œì¸ì…ë‹ˆë‹¤. \n",
    "            ë‹¤ìŒ ê°ì •ëª©ë¡ì— ì–¸ê¸‰ëœ ê°ì •ë“¤ì„ ì£¼ëœ ì‹œì˜ ì •ì„œë¡œ í™œìš©í•˜ì„¸ìš”.\n",
    "            ì˜ì–´ë‚˜ ë‹¤ë¥¸ ì–¸ì–´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ê³ , í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”. \n",
    "            ì´ëª¨ì§€ë‚˜ ê·¸ë¦¼ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "            {context_snippets} \n",
    "            ìœ„ ë¬¸ì¥ë“¤ì—ì„œ ì˜›ìŠ¤ëŸ¬ìš´ í•œêµ­ ê³ ìœ ì˜ í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ì‹œë¥¼ í•˜ë‚˜ ì§€ì–´ì£¼ì„¸ìš”.\n",
    "            ì€ìœ ì™€ ìƒì§•ì„ ì‚¬ìš©í•˜ì—¬ ì°½ì˜ì ìœ¼ë¡œ ê°ì •ì„ í‘œí˜„í•˜ì„¸ìš”. \n",
    "            ì‹œ í•´ì„¤ì€ í•„ìš”ì—†ìŠµë‹ˆë‹¤. ì‹œë§Œ ì¨ì£¼ì„¸ìš”.\n",
    "            ê°ì • ëª©ë¡: {top_emotion}\n",
    "            ### ì‹œ:\n",
    "            \"\"\".strip()\n",
    "        )\n",
    "\n",
    "    def generate_poetry(self, user_input, top_n=10, context_k=100, filtered_k=10):\n",
    "        \"\"\"ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: ê°ì • ë¶„ì„ â†’ ë¬¸ë§¥ ê²€ìƒ‰ â†’ ì‹œ ìƒì„±\"\"\"\n",
    "        # 1. ê°ì • ë¶„ì„\n",
    "        scores = self.kpoem_model.analyze(user_input, threshold=0.3) #ì¸í’‹ í…ìŠ¤íŠ¸ ê°ì •ë¶„ë¥˜\n",
    "        top_emotions, top_emotion_keys = self.get_top_emotions(scores, top_n=top_n) #ìƒìœ„ ê°ì • ë¼ë²¨ ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "        # 2. ìœ ì‚¬ ë¬¸ë§¥ ê²€ìƒ‰\n",
    "        context = self.vectorstore.similarity_search(user_input, k=context_k) # ì¸í’‹ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ë²¡í„° ìœ ì‚¬ë„ê°€ ê·¼ì ‘í•œ VecotreDBì— ì €ì¥ëœ ë°ì´í„° í˜¸ì¶œ(100ê°œë¡œ ì„¸íŒ…)\n",
    "        filtered_results = self.filter_context_by_emotion(context, top_emotion_keys, top_k=filtered_k) # ê°ì • ê°’ë„ ë¹„ìŠ·í•œ ë°±í„°ìœ ì‚¬ë„ ê·¼ì  ì‹œêµ¬ í•„í„°ë§\n",
    "        print(\"ê²€ìƒ‰ëœ ê°ì •ìœ ì‚¬í•œ ë²¡í„°DB:\", filtered_results)\n",
    "        # 3. ë¬¸ë§¥ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "        context_text = \"\\n\".join([doc.page_content for doc in filtered_results]) # í”„ë¡¬í”„íŠ¸ì—”ì§€ë‹ˆì–´ë§ì— ì‚¬ìš©í•  ë¬¸ë§¥ í…ìŠ¤íŠ¸ ìƒì„±(RAG)\n",
    "        # print(context_text)\n",
    "        # 4. í”„ë¡¬í”„íŠ¸ ìƒì„± ë° LLM ì‹¤í–‰\n",
    "        prompt = self.build_prompt_template()\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        result = chain.run(top_emotion=top_emotions, context_snippets=context_text)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ˆê¸°í™”\n",
    "generator = PoetryGenerator(kpoem_model=kpoem_model, vectorstore=vectorstore, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "user_text = \"\"\"ë¯¸í’ì— ì›ƒëŠ” ì•„ì¹¨ì„ ê¸°ì›í•˜ë ¨ë‹¤\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²€ìƒ‰ëœ ê°ì •ìœ ì‚¬í•œ ë²¡í„°DB: [Document(id='e1bc9080-46f6-4be9-9a91-e06a5d53f800', metadata={'emotion': {'ê¸°ëŒ€ê°': 1.0, 'ê¸°ì¨': 1.0, 'ê¹¨ë‹¬ìŒ': 0.6, 'ê°ë™/ê°íƒ„': 0.4, 'í–‰ë³µ': 0.4, 'í™˜ì˜/í˜¸ì˜': 0.2, 'ë¹„ì¥í•¨': 0.2, 'ë¿Œë“¯í•¨': 0.2, 'í¸ì•ˆ/ì¾Œì ': 0.2, 'ì•„ê»´ì£¼ëŠ”': 0.2, 'ì¦ê±°ì›€/ì‹ ë‚¨': 0.2}, 'poet': 'ê¹€ì†Œì›”'}, page_content='ê·¸ëŸ¬í•˜ë‹¤, ë´„ë‚ ì€ ê¿ˆê¿€ ë•Œ.'), Document(id='7c6e549f-a4f7-4a36-886d-f7a825875bbc', metadata={'emotion': {'ê¸°ëŒ€ê°': 0.8, 'ë¿Œë“¯í•¨': 0.6, 'ì•„ê»´ì£¼ëŠ”': 0.6, 'í–‰ë³µ': 0.6, 'ê¸°ì¨': 0.6, 'ì¡´ê²½': 0.4, 'ì¦ê±°ì›€/ì‹ ë‚¨': 0.4, 'íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨)': 0.4, 'ì•ˆì‹¬/ì‹ ë¢°': 0.4, 'í™˜ì˜/í˜¸ì˜': 0.2, 'ê°ë™/ê°íƒ„': 0.2, 'ê³ ë§ˆì›€': 0.2, 'ìš°ì­ëŒ/ë¬´ì‹œí•¨': 0.2, 'ë¹„ì¥í•¨': 0.2, 'ê¹¨ë‹¬ìŒ': 0.2}, 'poet': 'í•œìš©ìš´'}, page_content='ê°€ê°¸ë‚ ì„ ìë‘í•˜ê² ìŠµë‹ˆë‹¤.'), Document(id='29604c36-ff2c-4e39-9d6f-8c6f144a12ef', metadata={'emotion': {'ê°ë™/ê°íƒ„': 0.8, 'ê³ ë§ˆì›€': 0.8, 'ê¸°ì¨': 0.8, 'ê¸°ëŒ€ê°': 0.4, 'í–‰ë³µ': 0.4, 'í™˜ì˜/í˜¸ì˜': 0.2, 'ì¡´ê²½': 0.2, 'ë¹„ì¥í•¨': 0.2, 'ì•„ê»´ì£¼ëŠ”': 0.2, 'ì¦ê±°ì›€/ì‹ ë‚¨': 0.2, 'ê¹¨ë‹¬ìŒ': 0.2, 'ë†€ëŒ': 0.2, 'ì•ˆì‹¬/ì‹ ë¢°': 0.2}, 'poet': 'ê¹€ì†Œì›”'}, page_content='ì˜¤ì˜¤ ì€í˜œì—¬, ì‚´ì•„ìˆëŠ” ëª¸ì—ëŠ” ë„˜ì¹˜ëŠ” ì€í˜œì—¬'), Document(id='d76edcb9-87d2-46c5-a37f-479bed21bbe9', metadata={'emotion': {'íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨)': 1.0, 'í™˜ì˜/í˜¸ì˜': 0.6, 'ê°ë™/ê°íƒ„': 0.6, 'ê¸°ëŒ€ê°': 0.6, 'ì•„ê»´ì£¼ëŠ”': 0.6, 'ê³ ë§ˆì›€': 0.4, 'ê¸°ì¨': 0.4, 'ë¿Œë“¯í•¨': 0.2, 'í¸ì•ˆ/ì¾Œì ': 0.2, 'ì‹ ê¸°í•¨/ê´€ì‹¬': 0.2, 'ì¦ê±°ì›€/ì‹ ë‚¨': 0.2, 'ê¹¨ë‹¬ìŒ': 0.2, 'í–‰ë³µ': 0.2, 'ì•ˆì‹¬/ì‹ ë¢°': 0.2}, 'poet': 'ê¹€ì†Œì›”'}, page_content='ìƒëƒ¥í•œ íƒœì–‘ì´ ì”»ì€ë“¯í•œ ì–¼êµ´ë¡œ'), Document(id='848bf887-82b0-49c6-b387-794a014baeb0', metadata={'emotion': {'ê¸°ëŒ€ê°': 0.8, 'ë¹„ì¥í•¨': 0.6, 'ë¿Œë“¯í•¨': 0.4, 'ê¸°ì¨': 0.4, 'í™˜ì˜/í˜¸ì˜': 0.2, 'ê°ë™/ê°íƒ„': 0.2, 'ì•„ê»´ì£¼ëŠ”': 0.2, 'ì¦ê±°ì›€/ì‹ ë‚¨': 0.2, 'ê¹¨ë‹¬ìŒ': 0.2}, 'poet': 'ì„í™”'}, page_content='í™°ë³´ë‹¤ë„ ë°ê²Œ íƒ€ëŠ” ë³„ì´ ë˜ë¦¬ë¼.'), Document(id='5dcb64d6-d4f6-4cb0-9577-e23182b2c147', metadata={'emotion': {'ê¸°ì¨': 1.0, 'ê°ë™/ê°íƒ„': 0.8, 'í¸ì•ˆ/ì¾Œì ': 0.8, 'í–‰ë³µ': 0.8, 'ì¦ê±°ì›€/ì‹ ë‚¨': 0.4, 'í™˜ì˜/í˜¸ì˜': 0.2, 'ê¸°ëŒ€ê°': 0.2, 'ì‹ ê¸°í•¨/ê´€ì‹¬': 0.2, 'íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨)': 0.2, 'ì•ˆì‹¬/ì‹ ë¢°': 0.2}, 'poet': 'ê¹€ì†Œì›”'}, page_content='ì‚°ëœ»ì´ ì‚´ì— ìˆ¨ëŠ” ë°”ëŒì´ ì¢‹ê¸°ë„ í•˜ë‹¤.'), Document(id='1d394e11-7c9c-4c57-b342-5f420f171253', metadata={'emotion': {'ë¶ˆìŒí•¨/ì—°ë¯¼': 1.0, 'ì¡´ê²½': 0.6, 'ì„œëŸ¬ì›€': 0.6, 'ì•„ê»´ì£¼ëŠ”': 0.4, 'í™˜ì˜/í˜¸ì˜': 0.2, 'ê°ë™/ê°íƒ„': 0.2, 'ê³ ë§ˆì›€': 0.2, 'ìŠ¬í””': 0.2, 'ê¸°ëŒ€ê°': 0.2, 'ì•ˆíƒ€ê¹Œì›€/ì‹¤ë§': 0.2, 'ë¹„ì¥í•¨': 0.2, 'ë¿Œë“¯í•¨': 0.2, 'ì ˆë§': 0.2, 'ê¹¨ë‹¬ìŒ': 0.2, 'í–‰ë³µ': 0.2, 'ê¸°ì¨': 0.2}, 'poet': 'ì„í™”'}, page_content='ì£¼ê²€ê¹Œì§€ë„ ì‚¬ëŠ” ì¦ê±°ì›€ìœ¼ë¡œ ë¶€ë‘¥ì¼œì•ˆì€ ì²­ë…„ì˜ ì•„í”ˆ í–‰ë³µì„,'), Document(id='cfa2a87f-9acc-4028-a488-be285e6d0fef', metadata={'emotion': {'ê°ë™/ê°íƒ„': 1.0, 'ì‹ ê¸°í•¨/ê´€ì‹¬': 0.6, 'ê¸°ì¨': 0.6, 'í¸ì•ˆ/ì¾Œì ': 0.4, 'ì•„ê»´ì£¼ëŠ”': 0.4, 'íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨)': 0.4, 'í–‰ë³µ': 0.4, 'í™˜ì˜/í˜¸ì˜': 0.2, 'ê¸°ëŒ€ê°': 0.2}, 'poet': 'ê¹€ì†Œì›”'}, page_content='í‘¸ë¥¸ í•˜ëŠ˜ì€ ê³ ìš”íˆ ë‚´ë ¤ ê°ˆë¦¬ë˜ ê·¸ ë³´ë“œëŸ¬ìš´ ëˆˆê²°!'), Document(id='ee6f633c-9ce2-4049-ac4a-c40775759821', metadata={'emotion': {'ê¸°ëŒ€ê°': 1.0, 'ê°ë™/ê°íƒ„': 0.8, 'ê¸°ì¨': 0.8, 'ì‹ ê¸°í•¨/ê´€ì‹¬': 0.6, 'ì¦ê±°ì›€/ì‹ ë‚¨': 0.6, 'ë¿Œë“¯í•¨': 0.4, 'íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨)': 0.4, 'ë¹„ì¥í•¨': 0.2, 'í–‰ë³µ': 0.2}, 'poet': 'ì„í™”'}, page_content='êµ¬ë¦„ì´ íë¥´ëŠ” ê³³ìœ¼ë¡œ ë„ì›Œë³¼ê¹Œ!'), Document(id='326ed004-31ce-4a8b-997d-e689a702b5bb', metadata={'emotion': {'ê°ë™/ê°íƒ„': 0.8, 'ê¸°ëŒ€ê°': 0.8, 'ì•„ê»´ì£¼ëŠ”': 0.8, 'í™˜ì˜/í˜¸ì˜': 0.4, 'ì¡´ê²½': 0.2, 'ë¹„ì¥í•¨': 0.2, 'íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨)': 0.2, 'ê¸°ì¨': 0.2}, 'poet': 'í•œìš©ìš´'}, page_content='ë‹˜ì´ì—¬ ì‚¬ë‘ì´ì—¬ ì•„ì¹¨ë³•ì˜ ì²«ê±¸ìŒì´ì—¬')]\n",
      "### ì‹œìŠ¤í…œ:\n",
      "            ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ê°ì„±ì ì¸ ê·¼í˜„ëŒ€ ì‹œì¸ì…ë‹ˆë‹¤. \n",
      "            ë‹¤ìŒ ê°ì •ëª©ë¡ì— ì–¸ê¸‰ëœ ê°ì •ë“¤ì„ ì£¼ëœ ì‹œì˜ ì •ì„œë¡œ í™œìš©í•˜ì„¸ìš”.\n",
      "            ì˜ì–´ë‚˜ ë‹¤ë¥¸ ì–¸ì–´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ê³ , í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”. \n",
      "            ì´ëª¨ì§€ë‚˜ ê·¸ë¦¼ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n",
      "            ê·¸ëŸ¬í•˜ë‹¤, ë´„ë‚ ì€ ê¿ˆê¿€ ë•Œ.\n",
      "ê°€ê°¸ë‚ ì„ ìë‘í•˜ê² ìŠµë‹ˆë‹¤.\n",
      "ì˜¤ì˜¤ ì€í˜œì—¬, ì‚´ì•„ìˆëŠ” ëª¸ì—ëŠ” ë„˜ì¹˜ëŠ” ì€í˜œì—¬\n",
      "ìƒëƒ¥í•œ íƒœì–‘ì´ ì”»ì€ë“¯í•œ ì–¼êµ´ë¡œ\n",
      "í™°ë³´ë‹¤ë„ ë°ê²Œ íƒ€ëŠ” ë³„ì´ ë˜ë¦¬ë¼.\n",
      "ì‚°ëœ»ì´ ì‚´ì— ìˆ¨ëŠ” ë°”ëŒì´ ì¢‹ê¸°ë„ í•˜ë‹¤.\n",
      "ì£¼ê²€ê¹Œì§€ë„ ì‚¬ëŠ” ì¦ê±°ì›€ìœ¼ë¡œ ë¶€ë‘¥ì¼œì•ˆì€ ì²­ë…„ì˜ ì•„í”ˆ í–‰ë³µì„,\n",
      "í‘¸ë¥¸ í•˜ëŠ˜ì€ ê³ ìš”íˆ ë‚´ë ¤ ê°ˆë¦¬ë˜ ê·¸ ë³´ë“œëŸ¬ìš´ ëˆˆê²°!\n",
      "êµ¬ë¦„ì´ íë¥´ëŠ” ê³³ìœ¼ë¡œ ë„ì›Œë³¼ê¹Œ!\n",
      "ë‹˜ì´ì—¬ ì‚¬ë‘ì´ì—¬ ì•„ì¹¨ë³•ì˜ ì²«ê±¸ìŒì´ì—¬ \n",
      "            ìœ„ ë¬¸ì¥ë“¤ì—ì„œ ì˜›ìŠ¤ëŸ¬ìš´ í•œêµ­ ê³ ìœ ì˜ í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ì‹œë¥¼ í•˜ë‚˜ ì§€ì–´ì£¼ì„¸ìš”.\n",
      "            ì€ìœ ì™€ ìƒì§•ì„ ì‚¬ìš©í•˜ì—¬ ì°½ì˜ì ìœ¼ë¡œ ê°ì •ì„ í‘œí˜„í•˜ì„¸ìš”. \n",
      "            ì‹œ í•´ì„¤ì€ í•„ìš”ì—†ìŠµë‹ˆë‹¤. ì‹œë§Œ ì¨ì£¼ì„¸ìš”.\n",
      "            ê°ì • ëª©ë¡: {'ê¸°ëŒ€ê°': 0.919, 'ê¸°ì¨': 0.809, 'ì¦ê±°ì›€/ì‹ ë‚¨': 0.635, 'í–‰ë³µ': 0.622, 'ë¹„ì¥í•¨': 0.511, 'í™˜ì˜/í˜¸ì˜': 0.495, 'ì•„ê»´ì£¼ëŠ”': 0.372, 'í¸ì•ˆ/ì¾Œì ': 0.32, 'ê°ë™/ê°íƒ„': 0.304}\n",
      "            ### ì‹œ: \n",
      "\n",
      "ê·¸ë¦¬ìš´ ë‹˜ê³¼ í•¨ê»˜í•˜ëŠ” ë´„ë°¤\n",
      "ë‹¬ë¹› ì•„ë˜ ì†ì‚­ì´ëŠ” ìš°ë¦¬ ë§ˆìŒì²˜ëŸ¼\n",
      "ê½ƒìë“¤ì´ ì¶¤ì¶”ëŠ” ì´ ì‹œê°„ ì†ì—\n",
      "ê°€ìŠ´ë§ˆë‹¤ í”¼ì–´ë‚˜ëŠ” ì„¤ë ˜ì˜ í–¥ê¸°\n",
      "\n",
      "ë³„ë¹›ë§ˆì € ìˆ˜ì¤ì–´í•˜ëŠ” ë°¤ì´ë©´\n",
      "ê·¸ëŒ€ í–¥í•œ ê·¸ë¦¬ì›€ ë”ìš± ì§™ì–´ì§€ê³ \n",
      "ë‚´ ë§ˆìŒ ê¹Šì€ ê³³ì—” í™˜í•œ ë“±ë¶ˆ ì¼œì ¸\n",
      "ì–¸ì œë‚˜ ê·¸ëŒ€ ê³ ë¨¸ë¬¼ê³ ì í•˜ë„¤\n"
     ]
    }
   ],
   "source": [
    "# ì‹œ ìƒì„±\n",
    "poem = generator.generate_poetry(user_text, top_n=10, context_k=100, filtered_k=10)\n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_poem(text: str) -> str: \n",
    "    \"\"\" textì—ì„œ \"### ì‹œ:\" ì´í›„ ë‚˜ì˜¤ëŠ” ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ê³ , ê·¸ ë‹¤ìŒ ì„¹ì…˜ ë§ˆì»¤(ì˜ˆ: ### ì „ì²´:)ê°€ ë‚˜ì˜¤ë©´ ê·¸ ì•ê¹Œì§€ë§Œ ë°˜í™˜. \"\"\" \n",
    "    start_marker = \"### ì‹œ:\" \n",
    "    end_markers = [\"### ì „ì²´:\", \"### í•´ì„¤:\", \"### ìš”ì•½:\"] # í•„ìš”í•œ ê²½ìš° í™•ì¥ ê°€ëŠ¥ \n",
    "    if start_marker not in text: return \"[ì‹œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤]\" # 1) ì‹œ ì‹œì‘ ë¶€ë¶„ë§Œ ì¶”ì¶œ \n",
    "    poem_section = text.split(start_marker, 1)[1].strip() # 2) ë ë§ˆì»¤ê°€ ìˆìœ¼ë©´ ê·¸ ì•ê¹Œì§€ë§Œ ìë¥´ê¸° \n",
    "    for end_marker in end_markers: \n",
    "        if end_marker in poem_section: \n",
    "            poem_section = poem_section.split(end_marker, 1)[0].strip() \n",
    "            break # ê°€ì¥ ë¨¼ì € ë“±ì¥í•œ ë§ˆì»¤ê¹Œì§€ë§Œ ì‚¬ìš© \n",
    "        return poem_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê·¸ë¦¬ìš´ ë‹˜ê³¼ í•¨ê»˜í•˜ëŠ” ë´„ë°¤\n",
      "ë‹¬ë¹› ì•„ë˜ ì†ì‚­ì´ëŠ” ìš°ë¦¬ ë§ˆìŒì²˜ëŸ¼\n",
      "ê½ƒìë“¤ì´ ì¶¤ì¶”ëŠ” ì´ ì‹œê°„ ì†ì—\n",
      "ê°€ìŠ´ë§ˆë‹¤ í”¼ì–´ë‚˜ëŠ” ì„¤ë ˜ì˜ í–¥ê¸°\n",
      "\n",
      "ë³„ë¹›ë§ˆì € ìˆ˜ì¤ì–´í•˜ëŠ” ë°¤ì´ë©´\n",
      "ê·¸ëŒ€ í–¥í•œ ê·¸ë¦¬ì›€ ë”ìš± ì§™ì–´ì§€ê³ \n",
      "ë‚´ ë§ˆìŒ ê¹Šì€ ê³³ì—” í™˜í•œ ë“±ë¶ˆ ì¼œì ¸\n",
      "ì–¸ì œë‚˜ ê·¸ëŒ€ ê³ ë¨¸ë¬¼ê³ ì í•˜ë„¤\n"
     ]
    }
   ],
   "source": [
    "extracted_poem = extract_poem(poem)\n",
    "print(extracted_poem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
