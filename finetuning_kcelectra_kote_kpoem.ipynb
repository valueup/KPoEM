{"cells":[{"cell_type":"markdown","metadata":{"id":"Wqz4Tfmc8OKk"},"source":["# KcELECTRA + KOTE + KPoEM v3 모델 구현"]},{"cell_type":"markdown","metadata":{"id":"W287pi6VS4yk"},"source":["## 1. Basic setup and library imports\n","- Import Libraries and Set Configuration\n","  - Installs and imports required libraries (e.g., optuna, pytorch_lightning, transformers, etc.).\n","  - Sets random seed for reproducibility and configures model/data directories.\n","  - Defines constants such as number of epochs and input max length.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99k4AIQuS4yU","outputId":"7d7b4288-3df3-44cd-a8b0-8dfbc70a30df"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/bitsandbytes-0.45.4.dev0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n","\u001b[0m"]},{"name":"stderr","output_type":"stream","text":["Seed set to 42\n"]}],"source":["# ===================================================================\n","# 1. 기본 설정 및 라이브러리 임포트\n","# ===================================================================\n","!pip install -q optuna\n","\n","import os\n","import ast\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import pytorch_lightning as pl\n","import optuna\n","from datetime import datetime\n","from collections import Counter\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score, matthews_corrcoef, precision_score, recall_score\n","from torchmetrics.functional.classification import multilabel_accuracy\n","from tqdm.auto import tqdm\n","from IPython.display import display\n","\n","# PyTorch Lightning 로그 줄이기\n","import logging\n","logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n","\n","\n","# 시드 고정 및 환경 설정\n","RANDOM_SEED = 42\n","pl.seed_everything(RANDOM_SEED, workers=True)\n","torch.set_float32_matmul_precision('medium') # A100 등 TensorCore 사용 시 성능 향상\n","\n","# 경로 설정\n","DATA_DIR = '../data/'        # 상황에 맞게 조정\n","MODEL_SAVE_DIR = './model/'  # 상황에 맞게 조정\n","os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n","\n","# 학습 상수 설정\n","N_EPOCHS_OPTUNA = 3    # Optuna 탐색 시 사용할 Epochs\n","N_EPOCHS_TRAIN = 10    # 본 학습 시 사용할 Epochs\n","THRESHOLD = 0.3        # 고정 임계값\n","MAX_LENGTH = 512       # 토크나이저 최대 길이"]},{"cell_type":"markdown","source":["## 2. Data preparation (loading, preprocessing, splitting)\n","- Define Labels and Preprocessing Functions\n","  - Declares `LABELS`, a list of 44 Korean emotion categories.\n","  - `preprocess_by_paper_method()`: For KPoEM, aggregates labels from 5 annotators, applies min-max scaling to label agreement counts, and binarizes using a threshold.\n","  - `preprocess_kote()`: For KOTE, parses label indices from string to list and one-hot encodes them.\n","- Load and Split Datasets\n","  - Loads line-level and poem-level KPoEM data (`*.tsv` files), applies preprocessing, and splits into train/val/test sets.\n","  - Loads KOTE train/val/test splits, preprocesses them, and filters out rows with no labels."],"metadata":{"id":"hMm-BUtIkNoj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dHyqmDAuS4yk","outputId":"385cc4b2-f1b3-488d-b2b4-a94a7172b863"},"outputs":[{"name":"stdout","output_type":"stream","text":[">>> KPoEM 데이터셋 준비 중...\n","KPoEM 데이터셋: Train 6096, Val 763, Test 763\n","\n",">>> KOTE 데이터셋 준비 중 (로컬 tsv 파일 로드)...\n","KOTE 데이터셋 로드 및 전처리 완료: Train 40000, Val 5000, Test 5000\n"]}],"source":["# ===================================================================\n","# 2. 데이터 준비 (로드, 전처리, 분할) - KOTE 형식 반영 최종 수정\n","# ===================================================================\n","\n","# 2.1. 공통 전처리 함수 및 라벨 정의\n","LABELS = ['불평/불만', '환영/호의', '감동/감탄', '지긋지긋', '고마움', '슬픔', '화남/분노', '존경', '기대감', '우쭐댐/무시함', '안타까움/실망', '비장함', '의심/불신', '뿌듯함', '편안/쾌적', '신기함/관심', '아껴주는', '부끄러움', '공포/무서움', '절망', '한심함', '역겨움/징그러움', '짜증', '어이없음', '없음', '패배/자기혐오', '귀찮음', '힘듦/지침', '즐거움/신남', '깨달음', '죄책감', '증오/혐오', '흐뭇함(귀여움/예쁨)', '당황/난처', '경악', '부담/안_내킴', '서러움', '재미없음', '불쌍함/연민', '놀람', '행복', '불안/걱정', '기쁨', '안심/신뢰']\n","\n","def preprocess_by_paper_method(df: pd.DataFrame, threshold: float = 0.2) -> pd.DataFrame:\n","    \"\"\"\n","    KPoEM 데이터에 논문의 처리 방식을 적용\n","    1. 모든 평가자의 라벨을 취합\n","    2. 각 라벨의 등장 횟수(0~5)를 '점수'로 사용\n","    3. 점수를 Min-Max 스케일링하고 임계값을 적용해 최종 라벨 벡터를 생성\n","    \"\"\"\n","\n","    # 1. 모든 평가자의 라벨 취합\n","    # 각 행(댓글)에 대해 5명 평가자의 모든 감정 라벨을 하나의 리스트로 합침\n","    label_lists = []\n","    annotator_cols = [f'annotator0{i}' for i in range(1, 6) if f'annotator0{i}' in df.columns]\n","    if not annotator_cols: raise ValueError(\"KPoEM 데이터에 annotator 컬럼이 없습니다.\")\n","    for idx, row in df.iterrows():\n","        all_labels = []\n","        for col in annotator_cols: all_labels.extend(str(row[col]).split(','))\n","        label_lists.append([label.strip() for label in all_labels if label])\n","\n","    # 2. '동의 횟수'를 '점수'로 변환\n","    # 각 행별로 44개 감정에 대해 등장 횟수를 계산하여 점수 벡터(0~5점)를 생성\n","    score_vectors = [[Counter(comment_labels).get(label, 0) for label in LABELS] for comment_labels in label_lists]\n","    scores = np.array(score_vectors, dtype=float)\n","\n","    # 3. Min-Max 스케일링 및 이진화 (KOTE 논문 방식)\n","    min_scores, max_scores = scores.min(axis=1, keepdims=True), scores.max(axis=1, keepdims=True)\n","    numerator = scores - min_scores\n","    denominator = max_scores - min_scores\n","    scaled_scores = np.where(denominator != 0, numerator / denominator, 0)\n","    df['label_vector'] = [list(row) for row in (scaled_scores > threshold).astype(int)]\n","    return df\n","\n","# KOTE 전용 함수\n","def indices_to_vector(indices_data):\n","    vec = [0] * len(LABELS)\n","\n","    # 입력 데이터가 단일 숫자인지 확인하고, 맞으면 리스트로 감싸기\n","    if isinstance(indices_data, int):\n","        indices_list = [indices_data]\n","    else:\n","        indices_list = indices_data\n","\n","    # indices_list는 항상 리스트이므로 에러 없이 반복 가능\n","    if indices_list: # 리스트가 비어있지 않은 경우에만 실행\n","        for idx in indices_list:\n","            if 0 <= idx < len(LABELS):\n","                vec[idx] = 1\n","    return vec\n","\n","def preprocess_kote(df):\n","    df['label_indices'] = df['labels'].apply(ast.literal_eval)         # 'labels' 컬럼의 문자열(예: \"[5, 8]\")을 실제 리스트(예: [5, 8])로 변환\n","    df['label_vector'] = df['label_indices'].apply(indices_to_vector)  # 인덱스 리스트를 원-핫 인코딩 벡터로 변환\n","    df = df[df['label_vector'].apply(sum) > 0].reset_index(drop=True)\n","    return df\n","\n","\n","# 2.2. KPoEM 데이터 로드 및 분할\n","print(\">>> KPoEM 데이터셋 준비 중...\")\n","try:\n","    line_df = pd.read_csv(os.path.join(DATA_DIR, \"KPoEM_line_dataset_v3.tsv\"), sep='\\t')\n","    poem_df = pd.read_csv(os.path.join(DATA_DIR, \"KPoEM_poem_dataset_v3.tsv\"), sep='\\t')\n","\n","    line_df.rename(columns={'본문': 'text'}, inplace=True)\n","    poem_df.rename(columns={'본문': 'text'}, inplace=True)\n","\n","    line_df = preprocess_by_paper_method(line_df.copy(), threshold=0.2)  # 임계값 분포에 따른 수정\n","    poem_df = preprocess_by_paper_method(poem_df.copy(), threshold=0.2)  # 임계값 분포에 따른 수정\n","\n","    line_train_val, line_test = train_test_split(line_df, test_size=0.1, random_state=RANDOM_SEED)\n","    line_train, line_val = train_test_split(line_train_val, test_size=1/9, random_state=RANDOM_SEED)\n","    poem_train_val, poem_test = train_test_split(poem_df, test_size=0.1, random_state=RANDOM_SEED)\n","    poem_train, poem_val = train_test_split(poem_train_val, test_size=1/9, random_state=RANDOM_SEED)\n","\n","    kpoem_train_df = pd.concat([line_train, poem_train], ignore_index=True)\n","    kpoem_val_df = pd.concat([line_val, poem_val], ignore_index=True)\n","    kpoem_test_df = pd.concat([line_test, poem_test], ignore_index=True)\n","\n","    print(f\"KPoEM 데이터셋: Train {len(kpoem_train_df)}, Val {len(kpoem_val_df)}, Test {len(kpoem_test_df)}\")\n","except FileNotFoundError:\n","    print(f\"오류: KPoEM 데이터 파일을 '{os.path.join(DATA_DIR, 'KPoEM_...tsv')}' 경로에서 찾을 수 없습니다.\")\n","    kpoem_train_df, kpoem_val_df, kpoem_test_df = None, None, None\n","\n","\n","# 2.3. KOTE 데이터 로드 및 분할 (수정)\n","print(\"\\n>>> KOTE 데이터셋 준비 중 (로컬 tsv 파일 로드)...\")\n","try:\n","    kote_data_path = os.path.join(DATA_DIR, 'KOTE_dataset')\n","\n","    # tsv 파일에 컬럼명이 없으므로 지정\n","    kote_train_df = pd.read_csv(os.path.join(kote_data_path, 'train.tsv'), sep='\\t', header=None, names=['text', 'labels'])\n","    kote_val_df = pd.read_csv(os.path.join(kote_data_path, 'val.tsv'), sep='\\t', header=None, names=['text', 'labels'])\n","    kote_test_df = pd.read_csv(os.path.join(kote_data_path, 'test.tsv'), sep='\\t', header=None, names=['text', 'labels'])\n","\n","    # 각 데이터프레임에 새로 정의한 KOTE 전용 전처리 함수 적용\n","    kote_train_df = preprocess_kote(kote_train_df.copy())  # 수정\n","    kote_val_df = preprocess_kote(kote_val_df.copy())      # 수정\n","    kote_test_df = preprocess_kote(kote_test_df.copy())    # 수정\n","\n","    print(f\"KOTE 데이터셋 로드 및 전처리 완료: Train {len(kote_train_df)}, Val {len(kote_val_df)}, Test {len(kote_test_df)}\")\n","\n","except FileNotFoundError:\n","    print(f\"경고: KOTE tsv 파일을 '{os.path.join(DATA_DIR, 'KOTE_dataset')}' 디렉토리에서 찾을 수 없습니다.\")\n","    print(\"모델 B 학습을 건너뜁니다.\")\n","    kote_train_df, kote_val_df, kote_test_df = None, None, None"]},{"cell_type":"markdown","source":["## 3. Core components definition (Dataset, DataModule, LightningModule)\n","- Define Core Components (Model Pipeline)\n","  - Loads the tokenizer for KcELECTRA.\n","  - Defines `PoetryDataset` (PyTorch Dataset) and `PoetryDataModule` (Lightning DataModule).\n","  - Defines `BaseTagger`, a PyTorch Lightning module using KcELECTRA and a linear classifier with sigmoid for multi-label classification."],"metadata":{"id":"KNOv8Q53kRrB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fq_JrBNWS4yY"},"outputs":[],"source":["# ===================================================================\n","# 3. 코어 컴포넌트 정의 (Dataset, DataModule, LightningModule)\n","# ===================================================================\n","\n","# 3.1. 토크나이저 로드\n","MODEL_NAME = \"beomi/KcELECTRA-base\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","\n","# 3.2. Pytorch Dataset 정의\n","class PoetryDataset(Dataset):\n","    def __init__(self, df, tokenizer, max_length):\n","        self.texts = df['text'].tolist()\n","        self.labels = df['label_vector'].tolist()\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        labels = torch.FloatTensor(self.labels[idx])\n","        encoding = self.tokenizer(\n","            text,\n","            max_length=self.max_length,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_tensors=\"pt\"\n","        )\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': labels\n","        }\n","\n","\n","# 3.3. Pytorch Lightning DataModule 정의\n","class PoetryDataModule(pl.LightningDataModule):\n","    def __init__(self, train_df, val_df, test_df, tokenizer, batch_size=16, max_length=MAX_LENGTH):\n","        super().__init__()\n","        self.train_df = train_df\n","        self.val_df = val_df\n","        self.test_df = test_df\n","        self.tokenizer = tokenizer\n","        self.batch_size = batch_size\n","        self.max_length = max_length\n","        # Jupyter Notebook 환경에서는 num_workers를 0 또는 2로 설정하는 것이 안정적\n","        self.num_workers = 8 if torch.cuda.is_available() else 0  # 서버가 16코어라서 절반인 8로 설정\n","\n","    def setup(self, stage=None):\n","        if stage == 'fit' or stage is None:\n","            self.train_dataset = PoetryDataset(self.train_df, self.tokenizer, self.max_length)\n","            self.val_dataset = PoetryDataset(self.val_df, self.tokenizer, self.max_length)\n","        if stage == 'test' or stage is None:\n","            self.test_dataset = PoetryDataset(self.test_df, self.tokenizer, self.max_length)\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n","\n","    def test_dataloader(self):\n","        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n","\n","\n","# 3.4. Pytorch Lightning 모델(BaseTagger) 정의\n","class BaseTagger(pl.LightningModule):\n","    def __init__(self, model_name=MODEL_NAME, lr=2e-5, weight_decay=0.01,\n","                 n_training_steps=None, n_warmup_steps=None, dropout_rate=0.1):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.electra = AutoModel.from_pretrained(model_name)\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(p=self.hparams.dropout_rate),\n","            nn.Linear(self.electra.config.hidden_size, len(LABELS))\n","        )\n","        self.criterion = nn.BCELoss()\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        output = self.electra(input_ids, attention_mask=attention_mask)\n","        logits = self.classifier(output.last_hidden_state[:, 0, :])\n","        probs = torch.sigmoid(logits)\n","\n","        if labels is not None:\n","            loss = self.criterion(probs, labels)\n","            return loss, probs\n","        return None, probs\n","\n","    def training_step(self, batch, batch_idx):\n","        loss, _ = self(**batch)\n","        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        loss, _ = self(**batch)\n","        self.log(\"val_loss\", loss, prog_bar=True)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.AdamW(\n","            self.parameters(),\n","            lr=self.hparams.lr,\n","            weight_decay=self.hparams.weight_decay\n","        )\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=self.hparams.n_warmup_steps,\n","            num_training_steps=self.hparams.n_training_steps\n","        )\n","        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}}"]},{"cell_type":"markdown","source":["## Finetuning KcELECTRA <- KPoEM\n","- Experiment A: Fine-Tuning KcELECTRA on KPoEM\n","  - Uses Optuna to search for optimal hyperparameters (batch size, LR, dropout, etc.) using the KPoEM train/val split.\n","  - Fine-tunes the model using the best hyperparameters and saves the best checkpoint based on validation loss."],"metadata":{"id":"i_nuqxz9kUTh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"evCmuSITS4yl","outputId":"8127e6ea-8cfd-4ce6-a336-811e5cc08df9","colab":{"referenced_widgets":["4002a4fcae1a4af096dbbca7edbcb8dc","f726d6ac50644466afb34d79956b10ac","103534d6d1934c749c6debbf471e1339","5c1bdb01a1bf4e998cbee4db8998e925","fd128dc1e2db4b33b562c3bd052b9ff1","016bf5c413cc4ecb94c6b4bfa7cf40d4","afc37e1449d14bedbee3a97374a355ea","3306b77689764591b2a4e9d832d3c365","d9d844d432a34f2084f6609addc3119a","34fd1c80186242cba52330131b48c1b1","c36b87c162454518b8ec91ed1b8560be","2811ea3f6c7241c18ce44a7f18783457"]}},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2025-07-08 19:20:26,135] A new study created in memory with name: no-name-0e7424ca-e846-46ec-b0c2-94b1472319ea\n"]},{"name":"stdout","output_type":"stream","text":["==================================================\n","실험 A: KcELECTRA + KPoEM 시작 (minmax scaling 0.2)\n","==================================================\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-07-08 19:22:45,495] Trial 0 finished with value: 0.43985217809677124 and parameters: {'batch_size': 16, 'lr': 8.19085401122133e-06, 'weight_decay': 0.025082516844889866, 'dropout_rate': 0.2912337705470732}. Best is trial 0 with value: 0.43985217809677124.\n","[I 2025-07-08 19:25:05,044] Trial 1 finished with value: 0.3913053572177887 and parameters: {'batch_size': 16, 'lr': 4.583795544332515e-05, 'weight_decay': 4.726670962642425e-05, 'dropout_rate': 0.36805277239630296}. Best is trial 1 with value: 0.3913053572177887.\n","[I 2025-07-08 19:27:25,413] Trial 2 finished with value: 0.4588925540447235 and parameters: {'batch_size': 16, 'lr': 4.8793873560036606e-06, 'weight_decay': 0.00022971535906732196, 'dropout_rate': 0.18124480633556128}. Best is trial 1 with value: 0.3913053572177887.\n","[I 2025-07-08 19:30:19,967] Trial 3 finished with value: 0.47228384017944336 and parameters: {'batch_size': 8, 'lr': 2.6018763178450856e-06, 'weight_decay': 0.0001854715651303505, 'dropout_rate': 0.2383587096109655}. Best is trial 1 with value: 0.3913053572177887.\n","[I 2025-07-08 19:32:42,159] Trial 4 finished with value: 0.41475725173950195 and parameters: {'batch_size': 16, 'lr': 1.7847014110721008e-05, 'weight_decay': 0.00013159678474323499, 'dropout_rate': 0.41127587759912654}. Best is trial 1 with value: 0.3913053572177887.\n","[I 2025-07-08 19:35:39,989] Trial 5 finished with value: 0.4974118769168854 and parameters: {'batch_size': 8, 'lr': 1.1510124640949113e-06, 'weight_decay': 0.0278378146424558, 'dropout_rate': 0.21711256010931113}. Best is trial 1 with value: 0.3913053572177887.\n","[I 2025-07-08 19:38:03,890] Trial 6 finished with value: 0.5190689563751221 and parameters: {'batch_size': 16, 'lr': 1.4971708039459102e-06, 'weight_decay': 0.001481433345674508, 'dropout_rate': 0.39507821645833163}. Best is trial 1 with value: 0.3913053572177887.\n","[I 2025-07-08 19:40:28,834] Trial 7 finished with value: 0.48772573471069336 and parameters: {'batch_size': 16, 'lr': 2.6448684926454327e-06, 'weight_decay': 0.00017826795696389646, 'dropout_rate': 0.37394271718661876}. Best is trial 1 with value: 0.3913053572177887.\n","[I 2025-07-08 19:43:31,344] Trial 8 finished with value: 0.4117477238178253 and parameters: {'batch_size': 8, 'lr': 1.3647829871816353e-05, 'weight_decay': 8.132446229356162e-05, 'dropout_rate': 0.3277899604465735}. Best is trial 1 with value: 0.3913053572177887.\n","[I 2025-07-08 19:46:33,748] Trial 9 finished with value: 0.5040991902351379 and parameters: {'batch_size': 8, 'lr': 1.0163846879932094e-06, 'weight_decay': 0.00020753791324825102, 'dropout_rate': 0.29805870183430466}. Best is trial 1 with value: 0.3913053572177887.\n","[I 2025-07-08 19:49:04,852] Trial 10 finished with value: 0.3980351686477661 and parameters: {'batch_size': 16, 'lr': 4.17732892521744e-05, 'weight_decay': 1.0274168729139632e-05, 'dropout_rate': 0.4839992441693436}. Best is trial 1 with value: 0.3913053572177887.\n","[I 2025-07-08 19:51:32,497] Trial 11 finished with value: 0.39404523372650146 and parameters: {'batch_size': 16, 'lr': 4.377907945842921e-05, 'weight_decay': 1.0327191770661023e-05, 'dropout_rate': 0.48610284158947786}. Best is trial 1 with value: 0.3913053572177887.\n","[I 2025-07-08 19:53:59,453] Trial 12 finished with value: 0.3924185335636139 and parameters: {'batch_size': 16, 'lr': 4.900862407474735e-05, 'weight_decay': 1.3061003995632825e-05, 'dropout_rate': 0.49809809840739583}. Best is trial 1 with value: 0.3913053572177887.\n","[I 2025-07-08 19:56:26,314] Trial 13 finished with value: 0.40619781613349915 and parameters: {'batch_size': 16, 'lr': 2.1885016313735534e-05, 'weight_decay': 3.145022539540256e-05, 'dropout_rate': 0.12391666316548855}. Best is trial 1 with value: 0.3913053572177887.\n","[I 2025-07-08 19:58:53,911] Trial 14 finished with value: 0.40403124690055847 and parameters: {'batch_size': 16, 'lr': 3.0411529875961736e-05, 'weight_decay': 0.0014655393097250352, 'dropout_rate': 0.46879325800232874}. Best is trial 1 with value: 0.3913053572177887.\n"]},{"name":"stdout","output_type":"stream","text":["모델 A (KPoEM) 최적 하이퍼파라미터 (minmax scaling 0.2): {'batch_size': 16, 'lr': 4.583795544332515e-05, 'weight_decay': 4.726670962642425e-05, 'dropout_rate': 0.36805277239630296}\n","\n",">>> 모델 A 학습 시작...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/work/KPoEM/code/model/model_A exists and is not empty.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4002a4fcae1a4af096dbbca7edbcb8dc","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f726d6ac50644466afb34d79956b10ac","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"103534d6d1934c749c6debbf471e1339","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c1bdb01a1bf4e998cbee4db8998e925","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd128dc1e2db4b33b562c3bd052b9ff1","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"016bf5c413cc4ecb94c6b4bfa7cf40d4","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"afc37e1449d14bedbee3a97374a355ea","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3306b77689764591b2a4e9d832d3c365","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9d844d432a34f2084f6609addc3119a","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34fd1c80186242cba52330131b48c1b1","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c36b87c162454518b8ec91ed1b8560be","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2811ea3f6c7241c18ce44a7f18783457","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["모델 A 학습 완료 및 저장: /home/work/KPoEM/code/model/model_A/best_model_A_minmax_0.2.ckpt\n"]}],"source":["# ===================================================================\n","# 4. KcELECTRA + KPoEM\n","# ===================================================================\n","print(\"=\"*50)\n","print(\"실험 A: KcELECTRA + KPoEM 시작\")\n","print(\"=\"*50)\n","\n","# 4.1. KPoEM 최적 하이퍼파라미터 탐색 (Optuna)\n","def objective_kpoem(trial):\n","    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16])\n","    lr = trial.suggest_float(\"lr\", 1e-6, 5e-5, log=True)\n","    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-1, log=True)\n","    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n","\n","    data_module = PoetryDataModule(kpoem_train_df, kpoem_val_df, kpoem_test_df, tokenizer, batch_size=batch_size)\n","    steps_per_epoch = len(kpoem_train_df) // batch_size\n","    n_training_steps = steps_per_epoch * N_EPOCHS_OPTUNA\n","    n_warmup_steps = int(n_training_steps * 0.1)\n","\n","    model = BaseTagger(\n","        lr=lr, weight_decay=weight_decay, dropout_rate=dropout_rate,\n","        n_training_steps=n_training_steps, n_warmup_steps=n_warmup_steps\n","    )\n","    trainer = pl.Trainer(\n","        max_epochs=N_EPOCHS_OPTUNA, accelerator='gpu', devices=1,\n","        enable_checkpointing=False, logger=False, enable_progress_bar=False\n","    )\n","    trainer.fit(model, datamodule=data_module)\n","    return trainer.callback_metrics['val_loss'].item()\n","\n","study_kpoem = optuna.create_study(direction='minimize')\n","study_kpoem.optimize(objective_kpoem, n_trials=15) # n_trials는 필요에 따라 조절\n","best_hparams_A = study_kpoem.best_params\n","print(f\"모델 A (KPoEM) 최적 하이퍼파라미터: {best_hparams_A}\")\n","\n","\n","# 4.2. 모델 A 학습 및 저장\n","batch_size_A = best_hparams_A['batch_size']\n","dm_A = PoetryDataModule(kpoem_train_df, kpoem_val_df, kpoem_test_df, tokenizer, batch_size=batch_size_A)\n","steps_per_epoch_A = len(kpoem_train_df) // batch_size_A\n","n_training_steps_A = steps_per_epoch_A * N_EPOCHS_TRAIN\n","n_warmup_steps_A = int(n_training_steps_A * 0.1)\n","\n","model_A = BaseTagger(\n","    lr=best_hparams_A['lr'], weight_decay=best_hparams_A['weight_decay'], dropout_rate=best_hparams_A['dropout_rate'],\n","    n_training_steps=n_training_steps_A, n_warmup_steps=n_warmup_steps_A\n",")\n","\n","checkpoint_callback_A = pl.callbacks.ModelCheckpoint(\n","    dirpath=os.path.join(MODEL_SAVE_DIR, \"model_A\"), filename='best_model_A_minmax_0.2',\n","    save_top_k=1, verbose=False, monitor='val_loss', mode='min'\n",")\n","trainer_A = pl.Trainer(\n","    max_epochs=N_EPOCHS_TRAIN, accelerator='gpu', devices=1,\n","    callbacks=[checkpoint_callback_A], logger=False\n",")\n","\n","print(\"\\n>>> 모델 A 학습 시작...\")\n","trainer_A.fit(model_A, datamodule=dm_A)\n","best_model_A_path = checkpoint_callback_A.best_model_path\n","print(f\"모델 A 학습 완료 및 저장: {best_model_A_path}\")"]},{"cell_type":"markdown","source":["## 5. Finetuning KcELCTRA <- KOTE <- KPoEM\n","- Experiment B: Sequential Fine-Tuning (KOTE → KPoEM)\n","  - Stage 1: Fine-tunes on KOTE dataset using Optuna hyperparameter search.\n","    - Saves best model and optionally resumes from `last.ckpt` if it exists.\n","  - Stage 2: Loads the fine-tuned KOTE model and further fine-tunes it on KPoEM using new hyperparameters via Optuna.\n","    - Saves the best checkpoint after fine-tuning on KPoEM."],"metadata":{"id":"6HoGp0Hckpao"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DKLDtkRTS4yl","outputId":"508663b0-fcbd-46bd-a9cc-ab60941c47b9","colab":{"referenced_widgets":["402545bc52544a44be813e9cc55da99f","e79870e5455e4d6db7851fccba880fdb","c34973bc773d448395cda89205edd895","98c9e3bc7cce4d41bd7beda6b587df5f","4acb17fd33ac46498f300a8372a6229c","dba3f17117d54c5884a63f1a9cbe2b03","396d0a1b47304a5b8e5c0b042b65aef9","35fbe5040e7446819a0db38e18ec088c","51867b719d84426e945f24ba62dd3ef9","911576bb39fe4e8fbcfb03408f95161b","4f3853d20f694a869ef4cfb231e71a63","86dd097062cf4911ac2df5a5d49596a4","d7d22039e2f54a069bf8f94875dba53b","18462aebef7e46658bed30fab5a52778","e60bc088074b423bb3cc179a3e677660","83c83dd4303545cfab0aa58bf23fe4aa","25984cddabe64a1c8b73cbb46482b186","cfecbaacc59a449e9204bae90ce7293b","8927d7852c74474691ef0f7d622c3201","18331edca5d9482b87b8e72e366e14da","55ed93e1c95b4b12920adee10dc1932e","88d50656a54d4cafb4919066395d62c5","854601e073c8481d93a6f69f3ec20c81","a57b0be41b5d44c0ba249be1d7a6a2b5"]}},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2025-07-08 20:42:38,955] A new study created in memory with name: no-name-c8e2290c-6c22-4f6e-9693-9641c591adcf\n"]},{"name":"stdout","output_type":"stream","text":["\n","==================================================\n","실험 B: KcELECTRA -> KOTE -> KPoEM 시작 (minmax scaling 0.2)\n","==================================================\n","\n","5.1. 1차 파인튜닝 (on KOTE) (minmax scaling 0.2)\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-07-08 20:57:28,541] Trial 0 finished with value: 0.35989177227020264 and parameters: {'batch_size': 16, 'lr': 1.6636523243319953e-06, 'weight_decay': 0.04049257499328559, 'dropout_rate': 0.20913140887845133}. Best is trial 0 with value: 0.35989177227020264.\n","[I 2025-07-08 21:15:48,461] Trial 1 finished with value: 0.32227081060409546 and parameters: {'batch_size': 8, 'lr': 2.5937953170333734e-06, 'weight_decay': 0.0006172932751761212, 'dropout_rate': 0.23870797722576376}. Best is trial 1 with value: 0.32227081060409546.\n","[I 2025-07-08 21:34:17,640] Trial 2 finished with value: 0.3384235203266144 and parameters: {'batch_size': 8, 'lr': 1.6242310238629837e-06, 'weight_decay': 0.011536109777098756, 'dropout_rate': 0.10892700356790126}. Best is trial 1 with value: 0.32227081060409546.\n","[I 2025-07-08 21:52:50,604] Trial 3 finished with value: 0.3076414465904236 and parameters: {'batch_size': 8, 'lr': 3.996845164695625e-06, 'weight_decay': 0.025304545439822922, 'dropout_rate': 0.23485064341064948}. Best is trial 3 with value: 0.3076414465904236.\n","[I 2025-07-08 22:07:45,830] Trial 4 finished with value: 0.3765997588634491 and parameters: {'batch_size': 16, 'lr': 1.3002820163008242e-06, 'weight_decay': 0.0003893244402341264, 'dropout_rate': 0.41468171099196405}. Best is trial 3 with value: 0.3076414465904236.\n","[I 2025-07-08 22:22:42,876] Trial 5 finished with value: 0.28945741057395935 and parameters: {'batch_size': 16, 'lr': 1.4148056317304994e-05, 'weight_decay': 4.549397223140376e-05, 'dropout_rate': 0.4029701963226461}. Best is trial 5 with value: 0.28945741057395935.\n","[I 2025-07-08 22:37:42,247] Trial 6 finished with value: 0.3356740474700928 and parameters: {'batch_size': 16, 'lr': 3.3285572554670894e-06, 'weight_decay': 0.00044310132982855075, 'dropout_rate': 0.4635664808973956}. Best is trial 5 with value: 0.28945741057395935.\n","[I 2025-07-08 22:52:43,733] Trial 7 finished with value: 0.28075435757637024 and parameters: {'batch_size': 16, 'lr': 3.0935234989476135e-05, 'weight_decay': 0.002764544329463455, 'dropout_rate': 0.1312902247894547}. Best is trial 7 with value: 0.28075435757637024.\n","[I 2025-07-08 23:07:47,805] Trial 8 finished with value: 0.2812962532043457 and parameters: {'batch_size': 16, 'lr': 2.7960080281557406e-05, 'weight_decay': 2.6597630761354294e-05, 'dropout_rate': 0.1251230609335889}. Best is trial 7 with value: 0.28075435757637024.\n","[I 2025-07-08 23:22:54,830] Trial 9 finished with value: 0.3135506212711334 and parameters: {'batch_size': 16, 'lr': 5.0782342389088924e-06, 'weight_decay': 0.02954989900045934, 'dropout_rate': 0.2616151216359175}. Best is trial 7 with value: 0.28075435757637024.\n","[I 2025-07-08 23:41:42,814] Trial 10 finished with value: 0.281703382730484 and parameters: {'batch_size': 8, 'lr': 4.2908103658797905e-05, 'weight_decay': 0.0038149726350072887, 'dropout_rate': 0.3209188205529683}. Best is trial 7 with value: 0.28075435757637024.\n","[I 2025-07-08 23:56:52,700] Trial 11 finished with value: 0.2813207805156708 and parameters: {'batch_size': 16, 'lr': 3.77035993851182e-05, 'weight_decay': 1.5959279479127525e-05, 'dropout_rate': 0.11764452213435696}. Best is trial 7 with value: 0.28075435757637024.\n","[I 2025-07-09 00:12:04,709] Trial 12 finished with value: 0.2865675985813141 and parameters: {'batch_size': 16, 'lr': 1.562356703989158e-05, 'weight_decay': 8.65376112982488e-05, 'dropout_rate': 0.1651450643195004}. Best is trial 7 with value: 0.28075435757637024.\n","[I 2025-07-09 00:27:14,090] Trial 13 finished with value: 0.2845380902290344 and parameters: {'batch_size': 16, 'lr': 1.8089183697429036e-05, 'weight_decay': 0.0026896701945282532, 'dropout_rate': 0.16293158070374808}. Best is trial 7 with value: 0.28075435757637024.\n","[I 2025-07-09 00:42:23,941] Trial 14 finished with value: 0.28211984038352966 and parameters: {'batch_size': 16, 'lr': 2.64855753951486e-05, 'weight_decay': 1.0104064434948647e-05, 'dropout_rate': 0.31110765043453315}. Best is trial 7 with value: 0.28075435757637024.\n"]},{"name":"stdout","output_type":"stream","text":["\n",">>> 1차(KOTE) 모델 학습 시작 (minmax scaling 0.2)...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/work/KPoEM/code/model/kote_tuned exists and is not empty.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"402545bc52544a44be813e9cc55da99f","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e79870e5455e4d6db7851fccba880fdb","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c34973bc773d448395cda89205edd895","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98c9e3bc7cce4d41bd7beda6b587df5f","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4acb17fd33ac46498f300a8372a6229c","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dba3f17117d54c5884a63f1a9cbe2b03","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"396d0a1b47304a5b8e5c0b042b65aef9","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"35fbe5040e7446819a0db38e18ec088c","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51867b719d84426e945f24ba62dd3ef9","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"911576bb39fe4e8fbcfb03408f95161b","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f3853d20f694a869ef4cfb231e71a63","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86dd097062cf4911ac2df5a5d49596a4","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["1차(KOTE) 모델 (minmax scaling 0.2) 학습 완료 및 저장: /home/work/KPoEM/code/model/kote_tuned/kote_finetuned-epochepoch=02-val_lossval_loss=0.28_minmax_0.2.ckpt\n","\n","5.2. 2차 파인튜닝 (on KPoEM) (minmax scaling 0.2)\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-07-09 01:35:08,206] A new study created in RDB with name: kote-kpoem-hyper-tuning-minmax-0.2\n"]},{"name":"stdout","output_type":"stream","text":["모델 B Optuna 탐색을 이어합니다. (현재 0 / 목표 15)\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-07-09 01:38:22,566] Trial 0 finished with value: 0.4162595272064209 and parameters: {'batch_size': 8, 'lr': 3.732677906286875e-07, 'weight_decay': 0.0033507224632032787}. Best is trial 0 with value: 0.4162595272064209.\n","[I 2025-07-09 01:41:01,072] Trial 1 finished with value: 0.38702791929244995 and parameters: {'batch_size': 16, 'lr': 4.816803068102869e-06, 'weight_decay': 0.0006527326958403604}. Best is trial 1 with value: 0.38702791929244995.\n","[I 2025-07-09 01:43:39,826] Trial 2 finished with value: 0.4549036920070648 and parameters: {'batch_size': 16, 'lr': 1.2878408603689354e-07, 'weight_decay': 0.05048351948295225}. Best is trial 1 with value: 0.38702791929244995.\n","[I 2025-07-09 01:46:54,138] Trial 3 finished with value: 0.4349229335784912 and parameters: {'batch_size': 8, 'lr': 1.5400178504984476e-07, 'weight_decay': 2.5241785586075894e-05}. Best is trial 1 with value: 0.38702791929244995.\n","[I 2025-07-09 01:49:31,739] Trial 4 finished with value: 0.4547939896583557 and parameters: {'batch_size': 16, 'lr': 1.2936707043948062e-07, 'weight_decay': 0.000797585729706013}. Best is trial 1 with value: 0.38702791929244995.\n","[I 2025-07-09 01:52:42,837] Trial 5 finished with value: 0.4287770986557007 and parameters: {'batch_size': 8, 'lr': 1.98478474065362e-07, 'weight_decay': 0.00012387464334160975}. Best is trial 1 with value: 0.38702791929244995.\n","[I 2025-07-09 01:55:55,919] Trial 6 finished with value: 0.43225300312042236 and parameters: {'batch_size': 8, 'lr': 1.7034808159931002e-07, 'weight_decay': 0.06661026825178883}. Best is trial 1 with value: 0.38702791929244995.\n","[I 2025-07-09 01:59:07,623] Trial 7 finished with value: 0.39522290229797363 and parameters: {'batch_size': 8, 'lr': 1.548138901875164e-06, 'weight_decay': 0.0006961224994743178}. Best is trial 1 with value: 0.38702791929244995.\n","[I 2025-07-09 02:02:19,846] Trial 8 finished with value: 0.4035758376121521 and parameters: {'batch_size': 8, 'lr': 8.505416647969188e-07, 'weight_decay': 0.004295258981894335}. Best is trial 1 with value: 0.38702791929244995.\n","[I 2025-07-09 02:04:59,688] Trial 9 finished with value: 0.42740684747695923 and parameters: {'batch_size': 16, 'lr': 3.3237801394247155e-07, 'weight_decay': 0.00032748046769096403}. Best is trial 1 with value: 0.38702791929244995.\n","[I 2025-07-09 02:07:39,239] Trial 10 finished with value: 0.38154926896095276 and parameters: {'batch_size': 16, 'lr': 1.0700627114296843e-05, 'weight_decay': 1.1673832262229619e-05}. Best is trial 10 with value: 0.38154926896095276.\n","[I 2025-07-09 02:10:19,318] Trial 11 finished with value: 0.3817189931869507 and parameters: {'batch_size': 16, 'lr': 1.0276053289715197e-05, 'weight_decay': 1.899907944895087e-05}. Best is trial 10 with value: 0.38154926896095276.\n","[I 2025-07-09 02:12:59,221] Trial 12 finished with value: 0.38198548555374146 and parameters: {'batch_size': 16, 'lr': 9.965538139383563e-06, 'weight_decay': 1.1418834141169045e-05}. Best is trial 10 with value: 0.38154926896095276.\n","[I 2025-07-09 02:15:39,150] Trial 13 finished with value: 0.38030749559402466 and parameters: {'batch_size': 16, 'lr': 1.5681983616328386e-05, 'weight_decay': 5.183448757858411e-05}. Best is trial 13 with value: 0.38030749559402466.\n","[I 2025-07-09 02:18:19,637] Trial 14 finished with value: 0.3790091872215271 and parameters: {'batch_size': 16, 'lr': 1.8726765536618316e-05, 'weight_decay': 7.304905716726998e-05}. Best is trial 14 with value: 0.3790091872215271.\n"]},{"name":"stdout","output_type":"stream","text":["모델 B (KOTE->KPoEM) 최적 하이퍼파라미터: {'batch_size': 16, 'lr': 1.8726765536618316e-05, 'weight_decay': 7.304905716726998e-05} (minmax scaling 0.2)\n","\n",">>> 모델 B 최종 학습 시작 (minmax scaling 0.2)...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/work/KPoEM/code/model/model_B exists and is not empty.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7d22039e2f54a069bf8f94875dba53b","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18462aebef7e46658bed30fab5a52778","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e60bc088074b423bb3cc179a3e677660","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83c83dd4303545cfab0aa58bf23fe4aa","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"25984cddabe64a1c8b73cbb46482b186","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cfecbaacc59a449e9204bae90ce7293b","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8927d7852c74474691ef0f7d622c3201","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18331edca5d9482b87b8e72e366e14da","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"55ed93e1c95b4b12920adee10dc1932e","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88d50656a54d4cafb4919066395d62c5","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"854601e073c8481d93a6f69f3ec20c81","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a57b0be41b5d44c0ba249be1d7a6a2b5","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["모델 B (minmax scaling 0.2) 학습 완료 및 저장: /home/work/KPoEM/code/model/model_B/best_model_B_minmax_0.2.ckpt\n"]}],"source":["# ===================================================================\n","# 5. KcELECTRA -> KOTE -> KPoEM\n","# ===================================================================\n","if kote_train_df is not None and 'kpoem_train_df' in locals() and kpoem_train_df is not None:\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"실험 B: KcELECTRA -> KOTE -> KPoEM 시작\")\n","    print(\"=\"*50)\n","\n","\n","    # 5.1. 1차 파인튜닝 (KOTE 데이터)\n","    print(\"\\n5.1. 1차 파인튜닝 (on KOTE)\")\n","    def objective_kote(trial):\n","        batch_size = trial.suggest_categorical(\"batch_size\", [8, 16])\n","        lr = trial.suggest_float(\"lr\", 1e-6, 5e-5, log=True)\n","        weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-1, log=True)\n","        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n","\n","        data_module = PoetryDataModule(kote_train_df, kote_val_df, kote_test_df, tokenizer, batch_size=batch_size)\n","        steps_per_epoch = len(kote_train_df) // batch_size\n","        n_training_steps = steps_per_epoch * N_EPOCHS_OPTUNA\n","        n_warmup_steps = int(n_training_steps * 0.1)\n","\n","        model = BaseTagger(\n","            lr=lr, weight_decay=weight_decay, dropout_rate=dropout_rate,\n","            n_training_steps=n_training_steps, n_warmup_steps=n_warmup_steps\n","        )\n","        trainer = pl.Trainer(max_epochs=N_EPOCHS_OPTUNA, accelerator='gpu', devices=1, enable_checkpointing=False, logger=False, enable_progress_bar=False)\n","        trainer.fit(model, datamodule=data_module)\n","        return trainer.callback_metrics['val_loss'].item()\n","\n","    # optuna로 처음부터 찾는 경우\n","    study_kote = optuna.create_study(direction='minimize')\n","    study_kote.optimize(objective_kote, n_trials=15)\n","    best_hparams_kote = study_kote.best_params\n","\n","\n","    # 1차 파인튜닝 모델 학습\n","    batch_size_kote = best_hparams_kote['batch_size']\n","    dm_kote = PoetryDataModule(kote_train_df, kote_val_df, kote_test_df, tokenizer, batch_size=batch_size_kote)\n","    steps_per_epoch_kote = len(kote_train_df) // batch_size_kote\n","    n_training_steps_kote = steps_per_epoch_kote * N_EPOCHS_TRAIN\n","    n_warmup_steps_kote = int(n_training_steps_kote * 0.1)\n","\n","    model_kote_tuned = BaseTagger(\n","        lr=best_hparams_kote['lr'],\n","        weight_decay=best_hparams_kote['weight_decay'],\n","        dropout_rate=best_hparams_kote['dropout_rate'],\n","        n_training_steps=n_training_steps_kote,\n","        n_warmup_steps=n_warmup_steps_kote\n","    )\n","\n","    kote_ckpt_dir = os.path.join(MODEL_SAVE_DIR, \"kote_tuned\")\n","    checkpoint_callback_kote = pl.callbacks.ModelCheckpoint(\n","        dirpath=kote_ckpt_dir,\n","        filename='kote_finetuned-epoch{epoch:02d}-val_loss{val_loss:.2f}_minmax_0.2',  # val_loss도 파일명에 포함\n","        save_top_k=1,\n","        monitor='val_loss',\n","        mode='min',\n","        save_last=True\n","    )\n","    trainer_kote = pl.Trainer(max_epochs=N_EPOCHS_TRAIN, accelerator='gpu', devices=1, callbacks=[checkpoint_callback_kote], logger=False)\n","\n","    last_ckpt_path = os.path.join(kote_ckpt_dir, \"last_minmax.ckpt\")\n","    resume_path = last_ckpt_path if os.path.exists(last_ckpt_path) else None\n","\n","    if resume_path:\n","        print(f\"기존 KOTE 학습 체크포인트({resume_path})를 발견하여 이어서 학습합니다.\")\n","    else:\n","        print(\"\\n>>> 1차(KOTE) 모델 학습 시작...\")\n","\n","    trainer_kote.fit(model_kote_tuned, datamodule=dm_kote, ckpt_path=resume_path)\n","    kote_finetuned_ckpt_path = checkpoint_callback_kote.best_model_path\n","    print(f\"1차(KOTE) 모델 학습 완료 및 저장: {kote_finetuned_ckpt_path}\")\n","\n","\n","    # 5.2. 2차 파인튜닝 (KPoEM 데이터)\n","    print(\"\\n5.2. 2차 파인튜닝 (on KPoEM)\")\n","\n","    # objective 함수가 batch_size까지 새로 탐색하도록 수정\n","    ## KcELECTRA + KOTE에 맞는 새로운 hparams 탐색 필요!\n","    def objective_kote_kpoem(trial):\n","        # batch_size도 새로 탐색\n","        batch_size = trial.suggest_categorical(\"batch_size\", [8, 16])\n","        lr = trial.suggest_float(\"lr\", 1e-7, 2e-5, log=True)\n","        weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-1, log=True)\n","\n","        # 이번 trial의 batch_size로 데이터 모듈과 학습 스텝 새로 계산\n","        data_module = PoetryDataModule(kpoem_train_df, kpoem_val_df, kpoem_test_df, tokenizer, batch_size=batch_size)\n","        steps_per_epoch = len(kpoem_train_df) // batch_size\n","        n_training_steps = steps_per_epoch * N_EPOCHS_OPTUNA\n","        n_warmup_steps = int(n_training_steps * 0.1)\n","\n","        # 1차 학습된 모델을 불러와서 새 하이퍼파라미터로 학습\n","        model = BaseTagger.load_from_checkpoint(\n","            kote_finetuned_ckpt_path,\n","            lr=lr, weight_decay=weight_decay,\n","            n_training_steps=n_training_steps, # 새로 계산된 스텝 사용\n","            n_warmup_steps=n_warmup_steps      # 새로 계산된 스텝 사용\n","        )\n","        trainer = pl.Trainer(max_epochs=N_EPOCHS_OPTUNA, accelerator='gpu', devices=1, enable_checkpointing=False, logger=False, enable_progress_bar=False)\n","\n","        # 새로 만든 data_module을 사용\n","        trainer.fit(model, datamodule=data_module)\n","        return trainer.callback_metrics['val_loss'].item()\n","\n","    # Optuna 탐색 이어하기 기능 추가\n","    study_db_path_b = os.path.join(MODEL_SAVE_DIR, \"study_model_B_minmax_0.2.db\")\n","    study_name_b = \"kote-kpoem-hyper-tuning-minmax-0.2\"\n","    n_trials_b = 15\n","\n","    study_kote_kpoem = optuna.create_study(\n","        study_name=study_name_b,\n","        storage=f\"sqlite:///{study_db_path_b}\",\n","        direction='minimize',\n","        load_if_exists=True\n","    )\n","\n","    if len(study_kote_kpoem.trials) < n_trials_b:\n","        print(f\"모델 B Optuna 탐색을 이어합니다. (현재 {len(study_kote_kpoem.trials)} / 목표 {n_trials_b})\")\n","        study_kote_kpoem.optimize(objective_kote_kpoem, n_trials=(n_trials_b - len(study_kote_kpoem.trials)))\n","    else:\n","        print(\"모델 B Optuna 탐색이 이미 완료되었습니다.\")\n","\n","    best_hparams_B = study_kote_kpoem.best_params\n","    print(f\"모델 B (KOTE->KPoEM) 최적 하이퍼파라미터: {best_hparams_B}\")\n","\n","\n","    # 최종 모델 B 학습 및 저장\n","    batch_size_B = best_hparams_B['batch_size']\n","    dm_B_final = PoetryDataModule(kpoem_train_df, kpoem_val_df, kpoem_test_df, tokenizer, batch_size=batch_size_B)\n","    steps_per_epoch_B = len(kpoem_train_df) // batch_size_B\n","    n_training_steps_B = steps_per_epoch_B * N_EPOCHS_TRAIN\n","    n_warmup_steps_B = int(n_training_steps_B * 0.1)\n","\n","    model_B_final = BaseTagger.load_from_checkpoint(\n","        kote_finetuned_ckpt_path,\n","        lr=best_hparams_B['lr'], weight_decay=best_hparams_B['weight_decay'],\n","        n_training_steps=n_training_steps_B,  # 새로 계산된 스텝 B 사용\n","        n_warmup_steps=n_warmup_steps_B       # 새로 계산된 스텝 B 사용\n","    )\n","    # 최종 학습도 이어하기 가능하도록 수정\n","    model_b_ckpt_dir = os.path.join(MODEL_SAVE_DIR, \"model_B\")\n","    checkpoint_callback_B = pl.callbacks.ModelCheckpoint(\n","        dirpath=model_b_ckpt_dir,\n","        filename='best_model_B_minmax_0.2',\n","        save_top_k=1,\n","        verbose=False,\n","        monitor='val_loss',\n","        mode='min',\n","        save_last=True\n","    )\n","    trainer_B = pl.Trainer(\n","        max_epochs=N_EPOCHS_TRAIN, accelerator='gpu', devices=1,\n","        callbacks=[checkpoint_callback_B], logger=False\n","    )\n","\n","    last_ckpt_path_B = os.path.join(model_b_ckpt_dir, \"last_minmax_0.2.ckpt\")\n","    resume_path_B = last_ckpt_path_B if os.path.exists(last_ckpt_path_B) else None\n","\n","    if resume_path_B:\n","        print(f\"기존 모델 B 학습 체크포인트({resume_path_B})를 발견하여 이어서 학습합니다.\")\n","    else:\n","        print(\"\\n>>> 모델 B 최종 학습 시작...\")\n","\n","    # dm_A 대신 새로 만든 dm_B_final 사용 및 ckpt_path 추가\n","    trainer_B.fit(model_B_final, datamodule=dm_B_final, ckpt_path=resume_path_B)\n","    best_model_B_path = checkpoint_callback_B.best_model_path\n","    print(f\"모델 B 학습 완료 및 저장: {best_model_B_path}\")"]},{"cell_type":"markdown","source":["## 6. Final performance evaluation\n","- Final Evaluation\n","  - Loads all three best models:\n","    - A: KcELECTRA + KPoEM\n","    - B: KcELECTRA + KOTE → KPoEM\n","    - C: KcELECTRA + KOTE\n","  - Evaluates all models on the same KPoEM test set.\n","  - Computes and reports metrics: micro/macro precision, recall, F1, accuracy, and MCC.\n","  - Saves the results in a timestamped `.tsv` file."],"metadata":{"id":"e69U7_lXlJz4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ong7nKX9S4ym","outputId":"c29f7557-40da-4439-880d-c5d796ea32e2","colab":{"referenced_widgets":["88ab38d2bd3744c193e590c8a6e067ec","50a281b9db5b42f981237c4445623692","5323c9bab7e64bd69e850815393f75f1"]}},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","==================================================\n","최종 성능 비교\n","==================================================\n","\n",">>> 모델 A_minmax_0.2 평가 중...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88ab38d2bd3744c193e590c8a6e067ec","version_major":2,"version_minor":0},"text/plain":["Testing:   0%|          | 0/763 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n",">>> 모델 C_minmax_0.2 (KcELECTRA + KOTE) 평가 중...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"50a281b9db5b42f981237c4445623692","version_major":2,"version_minor":0},"text/plain":["Testing:   0%|          | 0/763 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n",">>> 모델 B_minmax_0.2 (KOTE -> KPoEM) 평가 중...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5323c9bab7e64bd69e850815393f75f1","version_major":2,"version_minor":0},"text/plain":["Testing:   0%|          | 0/763 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","최종 비교 결과\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Threshold</th>\n","      <th>Accuracy</th>\n","      <th>Precision_micro</th>\n","      <th>Precision_macro</th>\n","      <th>Recall_micro</th>\n","      <th>Recall_macro</th>\n","      <th>F1_micro</th>\n","      <th>F1_macro</th>\n","      <th>MCC</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A (KcELECTRA + KPoEM)</td>\n","      <td>0.3</td>\n","      <td>0.799089</td>\n","      <td>0.545235</td>\n","      <td>0.486193</td>\n","      <td>0.655391</td>\n","      <td>0.481596</td>\n","      <td>0.595260</td>\n","      <td>0.452580</td>\n","      <td>0.466600</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>C (KcELECTRA + KOTE)</td>\n","      <td>0.3</td>\n","      <td>0.769957</td>\n","      <td>0.486920</td>\n","      <td>0.461387</td>\n","      <td>0.381210</td>\n","      <td>0.330282</td>\n","      <td>0.427629</td>\n","      <td>0.343058</td>\n","      <td>0.289700</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>B (KOTE -&gt; KPoEM)</td>\n","      <td>0.3</td>\n","      <td>0.797629</td>\n","      <td>0.540987</td>\n","      <td>0.484663</td>\n","      <td>0.674947</td>\n","      <td>0.528997</td>\n","      <td>0.600588</td>\n","      <td>0.482592</td>\n","      <td>0.472393</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   Model  Threshold  Accuracy  Precision_micro  \\\n","0  A (KcELECTRA + KPoEM)        0.3  0.799089         0.545235   \n","1   C (KcELECTRA + KOTE)        0.3  0.769957         0.486920   \n","2      B (KOTE -> KPoEM)        0.3  0.797629         0.540987   \n","\n","   Precision_macro  Recall_micro  Recall_macro  F1_micro  F1_macro       MCC  \n","0         0.486193      0.655391      0.481596  0.595260  0.452580  0.466600  \n","1         0.461387      0.381210      0.330282  0.427629  0.343058  0.289700  \n","2         0.484663      0.674947      0.528997  0.600588  0.482592  0.472393  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","결과 파일 저장 완료: ./model/final_minmax_0.2_comparison_250709_195408.tsv\n"]}],"source":["# ===================================================================\n","# 6. 최종 성능 비교\n","# ===================================================================\n","import os\n","from sklearn.metrics import precision_score, recall_score\n","from torchmetrics.functional.classification import multilabel_accuracy\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"최종 성능 비교\")\n","print(\"=\"*50)\n","\n","# 저장된 모델 경로를 직접 지정\n","\n","# 모델 A의 'best' 체크포인트 경로\n","best_model_A_path = \"./model/model_A/best_model_A_minmax_0.2.ckpt\"\n","\n","# 모델 C (KOTE만 학습시킨 중간 모델)의 'best' 체크포인트 경로\n","kote_finetuned_ckpt_path = \"./model/kote_tuned/kote_finetuned-epochepoch=02-val_lossval_loss=0.28_minmax_0.2.ckpt\"\n","\n","# 모델 B의 'best' 체크포인트 경로\n","best_model_B_path = \"./model/model_B/best_model_B_minmax_0.2.ckpt\"\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 모델 평가 함수\n","def evaluate_model(model, test_dataset, device, threshold=THRESHOLD):\n","    model.to(device)\n","    model.eval()\n","    predictions_list = []\n","    labels_list = []\n","\n","    for item in tqdm(test_dataset, desc=\"Testing\"):\n","        input_ids = item[\"input_ids\"].to(device).unsqueeze(0)\n","        attention_mask = item[\"attention_mask\"].to(device).unsqueeze(0)\n","        labels = item[\"labels\"].to(device)\n","        with torch.no_grad():\n","            _, preds = model(input_ids, attention_mask)\n","        predictions_list.append(preds.flatten().cpu())\n","        labels_list.append(labels.cpu())\n","\n","    predictions_tensor = torch.stack(predictions_list)\n","    labels_tensor = torch.stack(labels_list)\n","    y_true = labels_tensor.numpy().astype(int)\n","    predictions_np = predictions_tensor.numpy()\n","    y_pred_bin = (predictions_np > threshold).astype(int)\n","\n","    # 성능 지표\n","    accuracy = multilabel_accuracy(torch.from_numpy(predictions_np), torch.from_numpy(y_true), num_labels=len(LABELS), threshold=threshold, average=\"micro\").item()\n","    f1_micro = f1_score(y_true, y_pred_bin, average='micro', zero_division=0)\n","    f1_macro = f1_score(y_true, y_pred_bin, average='macro', zero_division=0)\n","    mcc = matthews_corrcoef(y_true.flatten(), y_pred_bin.flatten())\n","    precision_micro = precision_score(y_true, y_pred_bin, average='micro', zero_division=0)\n","    recall_micro = recall_score(y_true, y_pred_bin, average='micro', zero_division=0)\n","    precision_macro = precision_score(y_true, y_pred_bin, average='macro', zero_division=0)\n","    recall_macro = recall_score(y_true, y_pred_bin, average='macro', zero_division=0)\n","\n","    # 반환 딕셔너리에 새로운 지표 추가\n","    return {\n","        \"Threshold\": threshold,\n","        \"Accuracy\": accuracy,\n","        \"Precision_micro\": precision_micro, \"Recall_micro\": recall_micro, \"F1_micro\": f1_micro,\n","        \"Precision_macro\": precision_macro, \"Recall_macro\": recall_macro, \"F1_macro\": f1_macro,\n","        \"MCC\": mcc\n","    }\n","\n","# 평가용 데이터셋 준비\n","if 'kpoem_test_df' in locals() and kpoem_test_df is not None:\n","    test_dataset = PoetryDataset(kpoem_test_df, tokenizer, MAX_LENGTH)\n","\n","    # 모델 평가 로직 (기존과 동일)\n","    print(\"\\n>>> 모델 A_minmax_0.2 평가 중...\")\n","    model_A_eval = BaseTagger.load_from_checkpoint(best_model_A_path)\n","    results_A = evaluate_model(model_A_eval, test_dataset, device)\n","    df_A = pd.DataFrame([results_A])\n","    df_A[\"Model\"] = \"A (KcELECTRA + KPoEM)\"\n","    df_list = [df_A]\n","\n","    if 'best_model_B_path' in locals() and best_model_B_path:\n","        print(\"\\n>>> 모델 C_minmax_0.2 (KcELECTRA + KOTE) 평가 중...\")\n","        model_C_eval = BaseTagger.load_from_checkpoint(kote_finetuned_ckpt_path)\n","        results_C = evaluate_model(model_C_eval, test_dataset, device)\n","        df_C = pd.DataFrame([results_C])\n","        df_C[\"Model\"] = \"C (KcELECTRA + KOTE)\"\n","        df_list.append(df_C)\n","\n","        print(\"\\n>>> 모델 B_minmax_0.2 (KOTE -> KPoEM) 평가 중...\")\n","        model_B_eval = BaseTagger.load_from_checkpoint(best_model_B_path)\n","        results_B = evaluate_model(model_B_eval, test_dataset, device)\n","        df_B = pd.DataFrame([results_B])\n","        df_B[\"Model\"] = \"B (KOTE -> KPoEM)\"\n","        df_list.append(df_B)\n","\n","    final_results_df = pd.concat(df_list, ignore_index=True)\n","\n","    # 최종 결과 출력 및 저장\n","    print(\"\\n최종 비교 결과\")\n","\n","    # display 할 컬럼 목록\n","    display_cols = [\n","        \"Model\", \"Threshold\",\n","        \"Accuracy\",\n","        \"Precision_micro\", \"Precision_macro\",\n","        \"Recall_micro\", \"Recall_macro\",\n","        \"F1_micro\", \"F1_macro\",\n","        \"MCC\"\n","    ]\n","    display(final_results_df[display_cols])\n","\n","    timestamp = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n","    save_path = os.path.join(MODEL_SAVE_DIR, f\"final_minmax_0.2_comparison_{timestamp}.tsv\")\n","    final_results_df.to_csv(save_path, sep='\\t', index=False)\n","    print(f\"\\n결과 파일 저장 완료: {save_path}\")\n","\n","else:\n","    print(\"KPoEM 테스트 데이터셋이 준비되지 않아 최종 평가를 건너뜁니다.\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}